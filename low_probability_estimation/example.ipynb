{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mgelu-1l\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m device = th.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m th.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m model = \u001b[43mTransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     29\u001b[39m dist_name = \u001b[33m\"\u001b[39m\u001b[33mcamel\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m gt_freqs = load_ground_truth(model_name, [dist_name], device=device)[dist_name] \u001b[38;5;66;03m# ground truth tensor\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/low_probability_estimation/low-probability-estimation/lpe/utils/transformer.py:158\u001b[39m, in \u001b[36mTransformer.from_pretrained\u001b[39m\u001b[34m(cls, model_name_or_path, checkpoint_path, **blobfile_kwargs)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m bf.BlobFile(state_dict_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m, **blobfile_kwargs) \u001b[38;5;28;01mas\u001b[39;00m state_dict_fh:\n\u001b[32m    157\u001b[39m     state_dict = th.load(state_dict_fh, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2573\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2566\u001b[39m         out = hook(module, incompatible_keys)\n\u001b[32m   2567\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   2568\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2569\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2570\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit should be done inplace.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2571\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2573\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[32m   2576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2561\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2555\u001b[39m         child_prefix = prefix + name + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2556\u001b[39m         child_state_dict = {\n\u001b[32m   2557\u001b[39m             k: v\n\u001b[32m   2558\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict.items()\n\u001b[32m   2559\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m k.startswith(child_prefix)\n\u001b[32m   2560\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m2561\u001b[39m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n\u001b[32m   2564\u001b[39m incompatible_keys = _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2544\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2542\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[32m   2543\u001b[39m     local_metadata[\u001b[33m\"\u001b[39m\u001b[33massign_to_params_buffers\u001b[39m\u001b[33m\"\u001b[39m] = assign\n\u001b[32m-> \u001b[39m\u001b[32m2544\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2548\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[43m    \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2551\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2552\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2553\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module._modules.items():\n\u001b[32m   2554\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2450\u001b[39m, in \u001b[36mModule._load_from_state_dict\u001b[39m\u001b[34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[39m\n\u001b[32m   2448\u001b[39m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[32m   2449\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2450\u001b[39m             \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2451\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m   2452\u001b[39m     action = \u001b[33m\"\u001b[39m\u001b[33mswapping\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcopying\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# If in colab, clone the repo and install the dependencies\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !git clone https://github.com/alignment-research-center/low-probability-estimation.git\n",
    "    import sys\n",
    "    sys.path.insert(0,'low-probability-estimation')\n",
    "\n",
    "    #install all packages in the install_requires of lpe\n",
    "    !pip install fancy-einsum\n",
    "    !pip install jaxtyping\n",
    "    !pip install datasets\n",
    "    !pip install blobfile\n",
    "\n",
    "    import os\n",
    "    os.environ[\"BLOBFILE_FORCE_GOOGLE_ANONYMOUS_AUTH\"] = \"1\"\n",
    "    \n",
    "import torch as th\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from lpe.methods import QLD, GLD, ITGIS, MHIS, gaussian_sampling_estimator\n",
    "from lpe.method_utils import *\n",
    "from lpe.utils import Transformer\n",
    "from lpe.utils import datasets as lpe_datasets\n",
    "\n",
    "model_name = \"gelu-1l\"\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = Transformer.from_pretrained(model_name).to(device)\n",
    "\n",
    "dist_name = \"camel\"\n",
    "gt_freqs = load_ground_truth(model_name, [dist_name], device=device)[dist_name] # ground truth tensor\n",
    "gt_probs = gt_freqs / gt_freqs.sum()\n",
    "targets = pick_random_tokens(gt_freqs, 16, 1e-9, 1e-5)\n",
    "\n",
    "## Uncomment this line to cache the dataset used to generate the distribution (recommended). It\n",
    "##   will save the dataset in your default HuggingFace cache directory and speed up the next cell.\n",
    "# lpe_datasets.USE_CACHE = True\n",
    "\n",
    "# Generate 2^16 samples of the pre-unembed activations (used for QLD and GLD)\n",
    "acts = gen_activ_samples(model, dist_name, n_samples=2**16, show_progress=True)\n",
    "\n",
    "# Generate estimates\n",
    "methods = [\"QLD\", \"ITGIS\", \"MHIS\", \"gaussian_sampling_estimator\"]\n",
    "estimates = {}\n",
    "orig_dists = distribution_registry[dist_name](model.tokenizer, device=model.device).input_dists(n_reps=N_REPS_DICT[dist_name])\n",
    "for method in methods:\n",
    "    print(f\"Computing estimates for {method}\")\n",
    "    estimates[method] = {}\n",
    "    for target in tqdm(list(targets)):\n",
    "        if method == \"QLD\":\n",
    "            estimates[method][target] = QLD(model.unembed.W_U, acts, target)\n",
    "        elif method == \"gaussian_sampling_estimator\":\n",
    "            estimates[method][target] = gaussian_sampling_estimator(model, acts, target)\n",
    "        elif method == \"ITGIS\":\n",
    "            estimates[method][target] = ITGIS(model, orig_dists, target, temp=RECOMMENDED_TEMPS[model_name][\"ITGIS\"][dist_name], n_samples=2**16)\n",
    "        elif method == \"MHIS\":\n",
    "            estimates[method][target] = MHIS(model, orig_dists, target, temp=RECOMMENDED_TEMPS[model_name][\"MHIS\"][dist_name], n_samples=2**16, burn_in=2**10)\n",
    "\n",
    "# Plot the (unfit) estimates\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = {'QLD': 'orange', 'ITGIS': 'red', 'MHIS': 'purple', 'gaussian_sampling_estimator': 'green'}\n",
    "for method in methods:\n",
    "    estimates_for_method = [estimates[method][target] for target in targets]\n",
    "    plt.scatter(gt_probs[targets].cpu().numpy(), estimates_for_method, label=method, color=colors[method])\n",
    "\n",
    "    # Plot the 0s at the bottom\n",
    "    zero_targets = list(filter(lambda target: estimates[method][target] == 0, targets))\n",
    "    plt.scatter(gt_probs[zero_targets].cpu().numpy(), [1e-9]*len(zero_targets), color=colors[method], marker='x')\n",
    "\n",
    "\n",
    "plt.plot([1e-9, 1e-5], [1e-9, 1e-5], label='ground truth', color='black')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Ground Truth Probability')\n",
    "plt.ylabel('Estimate')\n",
    "plt.title(f\"Estimates for {dist_name} on {model_name}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fancy-einsum in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (0.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jaxtyping in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (0.3.2)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from jaxtyping) (0.1.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (0.33.2)\n",
      "Requirement already satisfied: packaging in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from aiosignal>=1.1.2->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: blobfile in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (3.0.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from blobfile) (3.23.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from blobfile) (2.5.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from blobfile) (6.0.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from blobfile) (3.18.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (4.53.0)\n",
      "Requirement already satisfied: filelock in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages (from requests->transformers) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Install all packages in the install_requires of lpe\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'fancy-einsum'], check=True)\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'jaxtyping'], check=True)\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'datasets'], check=True)\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'blobfile'], check=True)\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'transformers'], check=True)\n",
    "os.environ[\"BLOBFILE_FORCE_GOOGLE_ANONYMOUS_AUTH\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from lpe.methods import QLD, GLD, ITGIS, MHIS, gaussian_sampling_estimator\n",
    "from lpe.method_utils import *\n",
    "from lpe.utils import Transformer\n",
    "from lpe.utils import datasets as lpe_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gelu-1l\"\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model = Transformer.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "import os\n",
    "\n",
    "# Set the certificate bundle path\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "\n",
    "# Your existing code\n",
    "dist_name = \"camel\"\n",
    "gt_freqs = load_ground_truth(model_name, [dist_name], device=device)[dist_name]\n",
    "gt_probs = gt_freqs / gt_freqs.sum()\n",
    "targets = pick_random_tokens(gt_freqs, 16, 1e-9, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 1024/1024 [00:35<00:00, 28.82it/s] \n",
      "100%|██████████| 1024/1024 [00:50<00:00, 20.42it/s]\n"
     ]
    }
   ],
   "source": [
    "## Uncomment this line to cache the dataset used to generate the distribution (recommended). It\n",
    "##   will save the dataset in your default HuggingFace cache directory and speed up the next cell.\n",
    "lpe_datasets.USE_CACHE = True\n",
    "\n",
    "# Generate 2^16 samples of the pre-unembed activations (used for QLD and GLD)\n",
    "acts = gen_activ_samples(model, dist_name, n_samples=2**16, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing estimates for GSE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 121\n",
      "    - L factor shape: torch.Size([512, 121])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1/16 [00:09<02:24,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 4010: 5.271701e-06 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 128\n",
      "    - L factor shape: torch.Size([512, 128])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 120\n",
      "    - L factor shape: torch.Size([512, 120])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 2/16 [00:14<01:32,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 17201: 1.606572e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 120\n",
      "    - L factor shape: torch.Size([512, 120])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 3/16 [00:18<01:11,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 1882: 2.602272e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 120\n",
      "    - L factor shape: torch.Size([512, 120])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 4/16 [00:22<01:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 17351: 1.779379e-06 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 121\n",
      "    - L factor shape: torch.Size([512, 121])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 5/16 [00:27<00:53,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 39665: 8.189541e-06 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 121\n",
      "    - L factor shape: torch.Size([512, 121])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 6/16 [00:32<00:49,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 11101: 1.485470e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 121\n",
      "    - L factor shape: torch.Size([512, 121])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 7/16 [00:37<00:44,  4.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 13855: 1.265527e-04 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 130\n",
      "    - L factor shape: torch.Size([512, 130])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 119\n",
      "    - L factor shape: torch.Size([512, 119])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:41<00:38,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 1305: 1.801085e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 122\n",
      "    - L factor shape: torch.Size([512, 122])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:46<00:33,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 2343: 3.750452e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 130\n",
      "    - L factor shape: torch.Size([512, 130])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 120\n",
      "    - L factor shape: torch.Size([512, 120])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 10/16 [00:51<00:28,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 44834: 7.478155e-07 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 130\n",
      "    - L factor shape: torch.Size([512, 130])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 122\n",
      "    - L factor shape: torch.Size([512, 122])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 11/16 [00:56<00:24,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 14274: 1.637997e-06 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 123\n",
      "    - L factor shape: torch.Size([512, 123])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 12/16 [01:01<00:19,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 16882: 3.038756e-06 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 121\n",
      "    - L factor shape: torch.Size([512, 121])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 13/16 [01:05<00:14,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 45896: 1.089080e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 129\n",
      "    - L factor shape: torch.Size([512, 129])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 122\n",
      "    - L factor shape: torch.Size([512, 122])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 14/16 [01:10<00:09,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 17699: 7.627693e-05 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 130\n",
      "    - L factor shape: torch.Size([512, 130])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 120\n",
      "    - L factor shape: torch.Size([512, 120])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 15/16 [01:15<00:04,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 30486: 2.244272e-06 ---\n",
      "HSE\n",
      "MODEL BLOCKS: ModuleList(\n",
      "  (0): TransformerBlock(\n",
      "    (ln1): LayerNorm()\n",
      "    (attn): Attention()\n",
      "    (ln2): LayerNorm()\n",
      "    (mlp): MLP()\n",
      "  )\n",
      ")\n",
      "Output Dim: 512\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 10000\n",
      "    - Drawn samples shape: torch.Size([10000, 512])\n",
      "    - Output samples shape after layer block: torch.Size([10000, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 128\n",
      "    - L factor shape: torch.Size([512, 128])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: TransformerBlock\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 262144 entries.\n",
      "    - Off-diagonal covariance matrix rank: 121\n",
      "    - L factor shape: torch.Size([512, 121])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "    - Propagating through layer: LayerNorm\n",
      "    - Input mean shape: torch.Size([512])\n",
      "    - Input covariance shape: torch.Size([512, 512])\n",
      "    - Number of samples for propagation: 512\n",
      "    - Drawn samples shape: torch.Size([512, 512])\n",
      "    - Output samples shape after layer block: torch.Size([512, 512])\n",
      "    - Output mean shape: torch.Size([512])\n",
      "    - Full output covariance shape: torch.Size([512, 512])\n",
      "    - Applying mask with 512 entries.\n",
      "    - Off-diagonal covariance matrix rank: 0\n",
      "    - Rank is 0, using diagonal remainder for covariance.\n",
      "    - L factor shape: torch.Size([512, 1])\n",
      "    - Diagonal remainder shape: torch.Size([512])\n",
      "  - Sampled final activations, shape: torch.Size([4096, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:19<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Computed final logits and probabilities, shape: torch.Size([4096, 48262])\n",
      "\n",
      "--- Estimated Probability for target 34313: 7.874703e-06 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "methods = [\"GSE\"]#, \"MHIS\", \"ITGIS\"] # Added GSE for Gaussian Sampling Estimator\n",
    "estimates = {}\n",
    "orig_dists = distribution_registry[dist_name](model.tokenizer, device=model.device).input_dists(n_reps=N_REPS_DICT[dist_name])\n",
    "for method in methods:\n",
    "    print(f\"Computing estimates for {method}\")\n",
    "    estimates[method] = {}\n",
    "    for target in tqdm(list(targets)):\n",
    "        if method == \"QLD\":\n",
    "            estimates[method][target] = QLD(model.unembed.W_U, acts, target)\n",
    "        elif method == \"ITGIS\":\n",
    "            estimates[method][target] = ITGIS(model, orig_dists, target, temp=RECOMMENDED_TEMPS[model_name][\"ITGIS\"][dist_name], n_samples=2**16)\n",
    "        elif method == \"MHIS\":\n",
    "            estimates[method][target] = MHIS(model, orig_dists, target, temp=RECOMMENDED_TEMPS[model_name][\"MHIS\"][dist_name], n_samples=2**16, burn_in=2**10)\n",
    "        elif method == \"GSE\":\n",
    "            print(\"HSE\")\n",
    "            estimates[method][target] = gaussian_sampling_estimator(\n",
    "                model, \n",
    "                orig_dists, \n",
    "                target, \n",
    "                n_samples=2**12,  # Or other desired number of samples\n",
    "                batch_size=512, # Or other desired batch size\n",
    "                n_off_diagonal_entries=1000000 # Number of off-diagonal covariance entries to include\n",
    "                )\n",
    "#31.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the (unfit) estimates\n",
    "plt.figure(figsize=(12, 6))\n",
    "colors = {'QLD': 'orange', 'ITGIS': 'red', 'MHIS': 'purple', 'GSE': 'green'}\n",
    "methods = [\"GSE\"]#, \"MHIS\", \"ITGIS\"]\n",
    "for method in methods:\n",
    "    estimates_for_method = [estimates[method][target] for target in targets]\n",
    "    plt.scatter(gt_probs[targets].cpu().numpy(), estimates_for_method, label=method, color=colors[method])\n",
    "\n",
    "    # Plot the 0s at the bottom\n",
    "    zero_targets = list(filter(lambda target: estimates[method][target] == 0, targets))\n",
    "    plt.scatter(gt_probs[zero_targets].cpu().numpy(), [1e-9]*len(zero_targets), color=colors[method], marker='x')\n",
    "\n",
    "\n",
    "plt.plot([1e-9, 1e-5], [1e-9, 1e-5], label='ground truth', color='black')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Ground Truth Probability')\n",
    "plt.ylabel('Estimate')\n",
    "plt.title(f\"Estimates for {dist_name} on {model_name}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Itakura-Saito loss for GSE: 4.042846080271166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'estimates', 'gt_probs', and 'targets' are from the notebook\n",
    "for method in methods:\n",
    "    total_loss = 0\n",
    "    num_targets = len(targets)\n",
    "    for target in targets:\n",
    "        p = gt_probs[target].item()\n",
    "        q = estimates[method][target]\n",
    "\n",
    "        # Avoid division by zero or log of zero for 0 estimates\n",
    "        if q == 0:\n",
    "            # Handle as a large loss, or skip\n",
    "            # Depending on how you want to treat 0-estimates.\n",
    "            # Here we will skip it and note it.\n",
    "            print(f\"Skipping target {target} for method {method} due to 0 estimate.\")\n",
    "            num_targets -=1\n",
    "            continue\n",
    "\n",
    "        loss = (p / q) - np.log(p / q) - 1\n",
    "        total_loss += loss\n",
    "\n",
    "    average_loss = total_loss / num_targets if num_targets > 0 else 0\n",
    "    print(f\"Average Itakura-Saito loss for {method}: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
