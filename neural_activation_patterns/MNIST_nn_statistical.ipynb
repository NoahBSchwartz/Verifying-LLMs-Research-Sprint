{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c27794c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "--- Starting Model Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  51%|█████     | 119/235 [00:02<00:02, 44.94it/s, accuracy=87.7, loss=0.382]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    173\u001b[39m model = SimpleNN()\n\u001b[32m    174\u001b[39m train_loader, train_dataset = get_train_loader(BATCH_SIZE)\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m eval_model_accuracy(model)\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# NAP Analysis\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, epochs)\u001b[39m\n\u001b[32m     42\u001b[39m total_preds = \u001b[32m0\u001b[39m\n\u001b[32m     44\u001b[39m progress_bar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:146\u001b[39m, in \u001b[36mMNIST.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    143\u001b[39m img = Image.fromarray(img.numpy(), mode=\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    149\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/estimating_nns.venv/lib/python3.11/site-packages/torchvision/transforms/functional.py:176\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.ByteTensor):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1000)\n",
    "        self.fc5 = nn.Linear(1000, 10)\n",
    "        \n",
    "        # Store ReLU layers for NAP analysis\n",
    "        self.relu_layers = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_preds += target.size(0)\n",
    "            correct_preds += (predicted == target).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=(running_loss / (batch_idx + 1)), \n",
    "                                   accuracy=100. * correct_preds / total_preds)\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "def get_train_loader(batch_size=256):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    loader_kwargs = {'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **loader_kwargs)\n",
    "    return train_loader, train_dataset\n",
    "\n",
    "def eval_model_accuracy(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "def get_all_activations(model, inputs):\n",
    "    \"\"\"Extract activations from both ReLU layers in SimpleNN\"\"\"\n",
    "    model.eval()\n",
    "    activations_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = inputs.view(-1, 28 * 28)\n",
    "        \n",
    "        # First layer activations\n",
    "        h1 = F.relu(model.fc1(x))\n",
    "        activations_list.append(h1)\n",
    "        \n",
    "        # Second layer activations  \n",
    "        h2 = F.relu(model.fc2(h1))\n",
    "        activations_list.append(h2)\n",
    "\n",
    "        h3 = F.relu(model.fc3(h2))\n",
    "        activations_list.append(h3)\n",
    "\n",
    "        h4 = F.relu(model.fc4(h3))\n",
    "        activations_list.append(h4)\n",
    "        \n",
    "    # Concatenate all activations\n",
    "    return torch.cat(activations_list, dim=1)\n",
    "\n",
    "def compute_input_nap(activations):\n",
    "    \"\"\"Convert activations to binary NAP\"\"\"\n",
    "    return (activations > 0).int()   \n",
    "\n",
    "def compute_baseline_nap(model, train_dataset, target_class, delta):\n",
    "    \"\"\"Compute baseline NAP specification for a target class\"\"\"\n",
    "    print(f\"\\nComputing baseline NAP specification for class {target_class} with delta={delta}...\")\n",
    "    \n",
    "    # Create a subset of the dataset for the target class\n",
    "    class_indices = [i for i, label in enumerate(train_dataset.targets) if label == target_class]\n",
    "    class_subset = Subset(train_dataset, class_indices)\n",
    "    class_loader = DataLoader(class_subset, batch_size=256, shuffle=False)\n",
    "    \n",
    "    all_class_naps = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(class_loader, desc=\"Extracting Activations\"):\n",
    "            inputs = inputs.to(device)\n",
    "            activations = get_all_activations(model, inputs)\n",
    "            binary_naps = compute_input_nap(activations)\n",
    "            all_class_naps.append(binary_naps.cpu())\n",
    "            \n",
    "    all_class_naps = torch.cat(all_class_naps, dim=0)\n",
    "    num_samples, num_neurons = all_class_naps.shape\n",
    "    \n",
    "    # Apply statistical abstraction (A_tilde) as per equation (6)\n",
    "    activation_counts = torch.sum(all_class_naps, dim=0)\n",
    "    deactivation_counts = num_samples - activation_counts\n",
    "    \n",
    "    baseline_nap = torch.full((num_neurons,), 2, dtype=torch.int)  # 2 represents '*'\n",
    "    \n",
    "    # Apply delta threshold\n",
    "    for i in range(num_neurons):\n",
    "        activation_freq = activation_counts[i].item() / num_samples\n",
    "        deactivation_freq = deactivation_counts[i].item() / num_samples\n",
    "        \n",
    "        if activation_freq >= delta:\n",
    "            baseline_nap[i] = 1  # State '1'\n",
    "        elif deactivation_freq >= delta:\n",
    "            baseline_nap[i] = 0  # State '0'\n",
    "        # Otherwise remains '*' (state 2)\n",
    "            \n",
    "    num_binary_neurons = (baseline_nap != 2).sum().item()\n",
    "    print(f\"Baseline NAP computed. Size (number of binary neurons): {num_binary_neurons}/{num_neurons}\")\n",
    "    return baseline_nap.to(device)\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "DELTA = 0.999  # Confidence ratio for the statistical NAP function (A_tilde)\n",
    "\n",
    "# Training\n",
    "print(\"--- Starting Model Training ---\")\n",
    "model = SimpleNN()\n",
    "train_loader, train_dataset = get_train_loader(BATCH_SIZE)\n",
    "train_model(model, train_loader, epochs=EPOCHS)\n",
    "eval_model_accuracy(model)\n",
    "\n",
    "# NAP Analysis\n",
    "print(f\"\\n=== Fitting Baseline NAPs for All Classes ===\")\n",
    "naps = []\n",
    "for target_class in range(10):\n",
    "    print(f\"\\n--- Processing Class {target_class} ---\")\n",
    "    class_nap = compute_baseline_nap(model, train_dataset, target_class, DELTA)\n",
    "    naps.append(class_nap)\n",
    "\n",
    "print(f\"\\n=== NAP Analysis Complete ===\")\n",
    "print(f\"Total classes processed: {len(naps)}\")\n",
    "for i, nap in enumerate(naps):\n",
    "    num_binary_neurons = (nap != 2).sum().item()\n",
    "    total_neurons = nap.shape[0]\n",
    "    print(f\"Class {i}: {num_binary_neurons}/{total_neurons} binary neurons\")\n",
    "\n",
    "# NAP Evaluation Functions\n",
    "def nap_subsumes(class_nap, input_nap):\n",
    "    return torch.all((class_nap == 2) | (class_nap == input_nap))\n",
    "\n",
    "# Test dataset setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "results = {i: [0, 0, 0, 0] for i in range(10)}\n",
    "\n",
    "print(\"\\n=== Running Final NAP Evaluation ===\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        activations = get_all_activations(model, inputs)\n",
    "        input_naps = compute_input_nap(activations)\n",
    "                  \n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for i in range(len(labels)):\n",
    "            true_label = labels[i].item()\n",
    "            predicted_label = predicted[i].item()\n",
    "            class_nap = naps[true_label]\n",
    "            input_nap = input_naps[i]\n",
    "            is_covered = nap_subsumes(class_nap, input_nap)\n",
    "            is_correct = (predicted_label == true_label)\n",
    "            if is_correct:\n",
    "                results[true_label][0] += 1\n",
    "                if is_covered:\n",
    "                    results[true_label][1] += 1\n",
    "            else:\n",
    "                results[true_label][2] += 1\n",
    "                if is_covered:\n",
    "                    results[true_label][3] += 1\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "for class_id, data in results.items():\n",
    "    correct_total, correct_covered, incorrect_total, incorrect_covered = data\n",
    "         \n",
    "    # Coverage over correct examples\n",
    "    coverage_correct = (correct_covered / correct_total) * 100 if correct_total > 0 else 0\n",
    "    # Coverage over incorrect examples\n",
    "    coverage_incorrect = (incorrect_covered / incorrect_total) * 100 if incorrect_total > 0 else 0\n",
    "         \n",
    "    print(f\"\\n--- Class {class_id} ---\")\n",
    "    print(f\"Coverage over correctly classified examples: {coverage_correct:.2f}% ({correct_covered}/{correct_total})\")\n",
    "    print(f\"Coverage over misclassified examples: {coverage_incorrect:.2f}% ({incorrect_covered}/{incorrect_total})\")\n",
    "\n",
    "print(\"\\n--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Results ---\n",
    "\n",
    "# --- Class 0 ---\n",
    "# Coverage over correctly classified examples: 78.70% (761/967)\n",
    "# Coverage over misclassified examples: 0.00% (0/13)\n",
    "\n",
    "# --- Class 1 ---\n",
    "# Coverage over correctly classified examples: 87.35% (987/1130)\n",
    "# Coverage over misclassified examples: 0.00% (0/5)\n",
    "\n",
    "# --- Class 2 ---\n",
    "# Coverage over correctly classified examples: 87.52% (884/1010)\n",
    "# Coverage over misclassified examples: 0.00% (0/22)\n",
    "\n",
    "# --- Class 3 ---\n",
    "# Coverage over correctly classified examples: 83.72% (828/989)\n",
    "# Coverage over misclassified examples: 0.00% (0/21)\n",
    "\n",
    "# --- Class 4 ---\n",
    "# Coverage over correctly classified examples: 84.47% (821/972)\n",
    "# Coverage over misclassified examples: 0.00% (0/10)\n",
    "\n",
    "# --- Class 5 ---\n",
    "# Coverage over correctly classified examples: 83.43% (730/875)\n",
    "# Coverage over misclassified examples: 0.00% (0/17)\n",
    "\n",
    "# --- Class 6 ---\n",
    "# Coverage over correctly classified examples: 78.76% (738/937)\n",
    "# Coverage over misclassified examples: 0.00% (0/21)\n",
    "\n",
    "# --- Class 7 ---\n",
    "# Coverage over correctly classified examples: 85.23% (860/1009)\n",
    "# Coverage over misclassified examples: 0.00% (0/19)\n",
    "\n",
    "# --- Class 8 ---\n",
    "# Coverage over correctly classified examples: 87.79% (820/934)\n",
    "# Coverage over misclassified examples: 2.50% (1/40)\n",
    "\n",
    "# --- Class 9 ---\n",
    "# Coverage over correctly classified examples: 86.61% (815/941)\n",
    "# Coverage over misclassified examples: 25.00% (17/68)\n",
    "\n",
    "# --------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estimating_nns.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
