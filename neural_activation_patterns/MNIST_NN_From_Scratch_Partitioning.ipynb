{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import heapq\n",
    "import itertools\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the number of random samples to use for training, calibration, and testing.\n",
    "# Calibration data is used to build the NAP tree.\n",
    "# Test data is used for the final evaluation.\n",
    "# Calibration and Test sets are disjoint.\n",
    "NUM_TRAIN_SAMPLES = 75\n",
    "NUM_CALIBRATION_SAMPLES = 8400\n",
    "NUM_TEST_SAMPLES = 1540\n",
    "\n",
    "\n",
    "# --- Model and Data Functions ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 28)\n",
    "        self.fc2 = nn.Linear(28, 28)\n",
    "        self.fc3 = nn.Linear(28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def get_subset_loaders(num_train, num_calibration, num_test, batch_size=256):\n",
    "    \"\"\"\n",
    "    Creates data loaders for training, calibration, and testing using random subsets of the MNIST dataset.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    loader_kwargs = {'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "\n",
    "    # Load full datasets\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # Create random subset for training\n",
    "    if num_train > len(train_dataset):\n",
    "        raise ValueError(f\"Requested {num_train} train samples, but only {len(train_dataset)} are available.\")\n",
    "    train_indices = np.random.choice(len(train_dataset), num_train, replace=False)\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, **loader_kwargs)\n",
    "\n",
    "    # Create disjoint random subsets for calibration and testing from the test set\n",
    "    if num_calibration + num_test > len(test_dataset):\n",
    "        raise ValueError(f\"The sum of calibration ({num_calibration}) and test ({num_test}) samples \"\n",
    "                         f\"cannot exceed the total test samples ({len(test_dataset)}).\")\n",
    "        \n",
    "    test_indices = np.arange(len(test_dataset))\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    calibration_indices = test_indices[:num_calibration]\n",
    "    test_indices_final = test_indices[num_calibration : num_calibration + num_test]\n",
    "\n",
    "    calibration_subset = Subset(test_dataset, calibration_indices)\n",
    "    test_subset = Subset(test_dataset, test_indices_final)\n",
    "\n",
    "    calibration_loader = DataLoader(calibration_subset, batch_size=batch_size, shuffle=False, **loader_kwargs)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, **loader_kwargs)\n",
    "\n",
    "    print(f\"Using {len(train_subset)} samples for training.\")\n",
    "    print(f\"Using {len(calibration_subset)} samples for calibration (NAP tree construction).\")\n",
    "    print(f\"Using {len(test_subset)} samples for final evaluation.\")\n",
    "\n",
    "    return train_loader, calibration_loader, test_loader\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, epochs=5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def extract_activations_and_losses(model, data_loader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_activations = []\n",
    "    all_losses = []\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            x = data.view(-1, 28 * 28)\n",
    "            h1 = F.relu(model.fc1(x))\n",
    "            h2 = F.relu(model.fc2(h1))\n",
    "            output = model.fc3(h2)\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            correct_mask = (predicted == target)\n",
    "\n",
    "            if not correct_mask.any():\n",
    "                continue\n",
    "\n",
    "            h1_correct = h1[correct_mask]\n",
    "            h2_correct = h2[correct_mask]\n",
    "            \n",
    "            h1_binary = (h1_correct > 0).float()\n",
    "            h2_binary = (h2_correct > 0).float()\n",
    "            nap = torch.cat([h1_binary, h2_binary], dim=1)\n",
    "            \n",
    "            all_activations.append(nap.cpu())\n",
    "            all_losses.append(criterion(output[correct_mask], target[correct_mask]).cpu())\n",
    "    \n",
    "    return torch.cat(all_activations).numpy(), torch.cat(all_losses).numpy()\n",
    "\n",
    "class NapNode:\n",
    "    id_iter = itertools.count()\n",
    "\n",
    "    def __init__(self, indices, parent=None, required=None, forbidden=None):\n",
    "        self.id = next(NapNode.id_iter)\n",
    "        self.indices = indices\n",
    "        self.parent = parent\n",
    "        \n",
    "        self.required = parent.required[:] if parent else []\n",
    "        if required is not None: self.required.append(required)\n",
    "        \n",
    "        self.forbidden = parent.forbidden[:] if parent else []\n",
    "        if forbidden is not None: self.forbidden.append(forbidden)\n",
    "        \n",
    "        self.children = []\n",
    "        self.is_leaf = True\n",
    "        self.variance = -1.0\n",
    "        self.sum_of_activations = None\n",
    "        self.sample_count = 0\n",
    "\n",
    "    def calculate_variance(self, losses):\n",
    "        if len(self.indices) < 2:\n",
    "            self.variance = 0.0\n",
    "        else:\n",
    "            self.variance = np.var(losses[self.indices])\n",
    "        return self.variance\n",
    "\n",
    "def get_all_test_data(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    all_activations, all_predicted, all_labels, all_losses = [], [], [], []\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            x = data.view(-1, 28 * 28)\n",
    "            h1 = F.relu(model.fc1(x))\n",
    "            h2 = F.relu(model.fc2(h1))\n",
    "            output = model.fc3(h2)\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            losses = criterion(output, target)\n",
    "\n",
    "            h1_binary = (h1 > 0).byte()\n",
    "            h2_binary = (h2 > 0).byte()\n",
    "            activations = torch.cat([h1_binary, h2_binary], dim=1)\n",
    "            \n",
    "            all_activations.append(activations.cpu().numpy())\n",
    "            all_predicted.append(predicted.cpu().numpy())\n",
    "            all_labels.append(target.cpu().numpy())\n",
    "            all_losses.append(losses.cpu().numpy())\n",
    "            \n",
    "    return (np.vstack(all_activations), \n",
    "            np.concatenate(all_predicted), \n",
    "            np.concatenate(all_labels), \n",
    "            np.concatenate(all_losses))\n",
    "\n",
    "def find_covering_leaf_node(activation, root_node):\n",
    "    current_node = root_node\n",
    "    while not current_node.is_leaf:\n",
    "        if not current_node.children:\n",
    "            break\n",
    "            \n",
    "        child1_constraints = set(current_node.children[0].required) | set(current_node.children[0].forbidden)\n",
    "        parent_constraints = set(current_node.required) | set(current_node.forbidden)\n",
    "        split_neuron_idx = (child1_constraints - parent_constraints).pop()\n",
    "\n",
    "        if activation[split_neuron_idx] == 1:\n",
    "            if split_neuron_idx in current_node.children[0].required:\n",
    "                current_node = current_node.children[0]\n",
    "            else:\n",
    "                current_node = current_node.children[1]\n",
    "        else:\n",
    "            if split_neuron_idx in current_node.children[0].forbidden:\n",
    "                current_node = current_node.children[0]\n",
    "            else:\n",
    "                current_node = current_node.children[1]\n",
    "    return current_node\n",
    "\n",
    "def nap_node_to_string(node):\n",
    "    if not node: return \"Inactive NAP\"\n",
    "    req_str = ','.join(map(str, sorted(node.required)))\n",
    "    forb_str = ','.join(map(str, sorted(node.forbidden)))\n",
    "    return f\"Required: [{req_str}] | Forbidden: [{forb_str}]\"\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Load Data\n",
    "train_loader, calibration_loader, test_loader = get_subset_loaders(\n",
    "    NUM_TRAIN_SAMPLES, NUM_CALIBRATION_SAMPLES, NUM_TEST_SAMPLES\n",
    ")\n",
    "\n",
    "# 2. Train Model\n",
    "model = SimpleNN()\n",
    "print(\"\\nTraining model...\")\n",
    "train_model(model, train_loader)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 3. Build NAP Tree\n",
    "print(\"\\nBuilding NAP tree using calibration data...\")\n",
    "correct_activations, correct_losses = extract_activations_and_losses(model, calibration_loader)\n",
    "\n",
    "estimation_steps = 100000\n",
    "start_time = time.time()\n",
    "\n",
    "root_node = NapNode(indices=np.arange(len(correct_activations)))\n",
    "root_node.sum_of_activations = np.sum(correct_activations, axis=0)\n",
    "root_node.sample_count = len(correct_activations)\n",
    "root_node.calculate_variance(correct_losses)\n",
    "\n",
    "pq = [(-root_node.variance, root_node.id)]\n",
    "heapq.heapify(pq)\n",
    "\n",
    "tree_nodes = {root_node.id: root_node}\n",
    "leaf_count = 1\n",
    "\n",
    "# Tracking variables for split termination reasons\n",
    "stopped_insufficient_samples = 0\n",
    "stopped_no_discriminator = 0\n",
    "stopped_no_median_split = 0\n",
    "\n",
    "for estimation_step in range(estimation_steps):\n",
    "    if not pq:\n",
    "        break\n",
    "\n",
    "    _, node_id_to_split = heapq.heappop(pq)\n",
    "    parent_node = tree_nodes[node_id_to_split]\n",
    "\n",
    "    if not parent_node.is_leaf:\n",
    "        continue\n",
    "        \n",
    "    if len(parent_node.indices) < 2:\n",
    "        stopped_insufficient_samples += 1\n",
    "        continue\n",
    "\n",
    "    parent_indices = parent_node.indices\n",
    "    losses_in_nap = correct_losses[parent_indices]\n",
    "\n",
    "    loss_median = np.median(losses_in_nap)\n",
    "    high_loss_mask = (losses_in_nap > loss_median)\n",
    "    low_loss_mask = ~high_loss_mask\n",
    "\n",
    "    if not np.any(high_loss_mask) or not np.any(low_loss_mask):\n",
    "        stopped_no_median_split += 1\n",
    "        continue\n",
    "\n",
    "    activations_in_nap = correct_activations[parent_indices]\n",
    "    \n",
    "    freq_high = np.mean(activations_in_nap[high_loss_mask], axis=0)\n",
    "    freq_low = np.mean(activations_in_nap[low_loss_mask], axis=0)\n",
    "    discriminative_scores = np.abs(freq_high - freq_low)\n",
    "    \n",
    "    used_neurons = parent_node.required + parent_node.forbidden\n",
    "    if used_neurons:\n",
    "      discriminative_scores[used_neurons] = -1\n",
    "        \n",
    "    best_neuron_idx = np.argmax(discriminative_scores)\n",
    "    \n",
    "    if discriminative_scores[best_neuron_idx] < 0:\n",
    "        stopped_no_discriminator += 1\n",
    "        continue\n",
    "\n",
    "    parent_node.is_leaf = False\n",
    "    leaf_count -= 1\n",
    "    \n",
    "    guide_activation_value = activations_in_nap[low_loss_mask][0, best_neuron_idx]\n",
    "    \n",
    "    split_column = activations_in_nap[:, best_neuron_idx]\n",
    "    mask1 = (split_column == guide_activation_value)\n",
    "    mask2 = ~mask1\n",
    "\n",
    "    if guide_activation_value == 1:\n",
    "        child1 = NapNode(indices=parent_indices[mask1], parent=parent_node, required=best_neuron_idx)\n",
    "        child2 = NapNode(indices=parent_indices[mask2], parent=parent_node, forbidden=best_neuron_idx)\n",
    "    else:\n",
    "        child1 = NapNode(indices=parent_indices[mask1], parent=parent_node, forbidden=best_neuron_idx)\n",
    "        child2 = NapNode(indices=parent_indices[mask2], parent=parent_node, required=best_neuron_idx)\n",
    "    \n",
    "    parent_node.children = [child1, child2]\n",
    "\n",
    "    if len(child1.indices) <= len(child2.indices):\n",
    "        small_child, large_child = child1, child2\n",
    "    else:\n",
    "        small_child, large_child = child2, child1\n",
    "\n",
    "    small_child.sum_of_activations = np.sum(correct_activations[small_child.indices], axis=0)\n",
    "    small_child.sample_count = len(small_child.indices)\n",
    "\n",
    "    large_child.sum_of_activations = parent_node.sum_of_activations - small_child.sum_of_activations\n",
    "    large_child.sample_count = parent_node.sample_count - small_child.sample_count\n",
    "    \n",
    "    for child in parent_node.children:\n",
    "        tree_nodes[child.id] = child\n",
    "        if len(child.indices) >= 2:\n",
    "            child.calculate_variance(correct_losses)\n",
    "            heapq.heappush(pq, (-child.variance, child.id))\n",
    "            leaf_count += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
