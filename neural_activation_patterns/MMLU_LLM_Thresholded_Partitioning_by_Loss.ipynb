{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from bidict import bidict\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_saes(device, total_layers=26):\n",
    "    \"\"\"Loads a series of Sparse Autoencoders (SAEs) for specified layers\"\"\"\n",
    "    saes = []\n",
    "    print(f\"Loading {total_layers} SAEs...\")\n",
    "    for layer in range(total_layers):\n",
    "        sae, _, _ = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "            device=device\n",
    "        )\n",
    "        saes.append(sae)\n",
    "        print(f\"Layer {layer} loaded.\")\n",
    "    return saes\n",
    "\n",
    "def get_all_mmlu_questions(dataset):\n",
    "    \"\"\"Returns all questions from the MMLU dataset\"\"\"\n",
    "    all_questions = []\n",
    "    for i in range(len(dataset)):\n",
    "        data_point = dataset[i]\n",
    "        question = data_point['question']\n",
    "        choices = data_point['choices']\n",
    "        instruction = \"The following is a multiple choice question. Output only a single token corresponding to the right answer (ie: A) \\n\"\n",
    "        formatted_question = instruction + f\" Question: {question}\\n\"\n",
    "        choice_labels = ['A', 'B', 'C', 'D']\n",
    "        for j, choice in enumerate(choices):\n",
    "            formatted_question += f\"{choice_labels[j]}) {choice}\\n\"\n",
    "        formatted_question += \"Answer: \"\n",
    "        all_questions.append({\n",
    "            'text': formatted_question,\n",
    "            'subject': data_point['subject'],\n",
    "            'answer': data_point['answer'],\n",
    "            'choices': data_point['choices'],\n",
    "            'raw_question': question\n",
    "        })\n",
    "    return all_questions\n",
    "        \n",
    "def process_question(model, saes, question, feature_bidict, sample_idx=None):\n",
    "    \"\"\"\n",
    "    Performs a forward pass, extracts features and loss for a single MMLU question\n",
    "    \"\"\"\n",
    "    question_text = question['text']\n",
    "    correct_answer_idx = question['answer']\n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    total_features = len(feature_bidict)\n",
    "    feature_vector = np.zeros(total_features, dtype=np.byte)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tokens = model.to_tokens(question_text, prepend_bos=True).to(model.cfg.device)\n",
    "        logits, cache = model.run_with_cache(input_tokens)\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        choice_token_ids = [model.to_tokens(label, prepend_bos=False)[0, 0].item() for label in choice_labels]\n",
    "        choice_logits = last_token_logits[choice_token_ids]\n",
    "        loss = F.cross_entropy(choice_logits.unsqueeze(0), torch.tensor([correct_answer_idx]).to(model.cfg.device)).item()\n",
    "        predicted_choice_idx = torch.argmax(choice_logits).item()\n",
    "        is_correct = predicted_choice_idx == correct_answer_idx\n",
    "\n",
    "        print(f\"\\n--- Sample {sample_idx} ---\")\n",
    "        print(f\"Subject: {question['subject']}\")\n",
    "        print(f\"Question: {question['raw_question']}\")\n",
    "        print(\"Choices:\")\n",
    "        for i, choice in enumerate(question['choices']):\n",
    "            print(f\"  {choice_labels[i]}) {choice}\")\n",
    "        print(f\"Correct Answer: {choice_labels[correct_answer_idx]} ({question['choices'][correct_answer_idx]})\")\n",
    "        print(f\"Model's Answer: {choice_labels[predicted_choice_idx]} ({question['choices'][predicted_choice_idx]})\")\n",
    "        print(f\"Result: {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "        for layer_idx, sae in enumerate(saes):\n",
    "            final_token_activations = cache[sae.cfg.hook_name][0, -1, :].unsqueeze(0)\n",
    "            feature_acts = sae.encode(final_token_activations)\n",
    "            active_indices = torch.where(feature_acts > 0)[1].cpu().tolist()\n",
    "            for feature_idx in active_indices:\n",
    "                global_feature_idx = feature_bidict.get((layer_idx, feature_idx))\n",
    "                if global_feature_idx is not None:\n",
    "                    feature_vector[global_feature_idx] = 1\n",
    "\n",
    "    return feature_vector, is_correct\n",
    "        \n",
    "def extract_features_and_correctness(model, saes, questions, feature_bidict, output_dir=None):\n",
    "    \"\"\"\n",
    "    Processes questions to get features and correctness, with an option to save data\n",
    "    \"\"\"\n",
    "    all_feature_vectors = []\n",
    "    all_correctness_labels = []\n",
    "    for i, question in enumerate(questions):\n",
    "        feature_vector, is_correct = process_question(\n",
    "            model, saes, question, feature_bidict, sample_idx=i+1\n",
    "        )\n",
    "        all_feature_vectors.append(feature_vector)\n",
    "        all_correctness_labels.append(is_correct)\n",
    "\n",
    "    if output_dir:\n",
    "        features_np = np.array(all_feature_vectors, dtype=np.byte)\n",
    "        correctness_np = np.array(all_correctness_labels)\n",
    "        feature_path = os.path.join(output_dir, f\"features.npy\")\n",
    "        correctness_path = os.path.join(output_dir, f\"correctness.npy\")\n",
    "        np.save(feature_path, features_np)\n",
    "        np.save(correctness_path, correctness_np)\n",
    "\n",
    "    return np.array(all_feature_vectors, dtype=np.byte), np.array(all_correctness_labels)\n",
    "\n",
    "def load_or_create_data(model, saes, feature_bidict, mmlu_dataset, output_dir):\n",
    "    feature_path = os.path.join(output_dir, \"features.npy\")\n",
    "    correctness_path = os.path.join(output_dir, \"correctness.npy\")\n",
    "    if os.path.exists(feature_path) and os.path.exists(correctness_path):\n",
    "        print(f\"Found cached data. Loading from '{output_dir}'...\")\n",
    "        feature_vectors = np.load(feature_path)\n",
    "        correctness_labels = np.load(correctness_path)\n",
    "    else:\n",
    "        print(f\"No cached data found in '{output_dir}'. Generating data from scratch...\")\n",
    "        questions = get_all_mmlu_questions(mmlu_dataset)\n",
    "        feature_vectors, correctness_labels = extract_features_and_correctness(\n",
    "            model, saes, questions, feature_bidict,\n",
    "            output_dir=output_dir)\n",
    "    return feature_vectors, correctness_labels\n",
    "\n",
    "\n",
    "# Init all variables and load SAEs for all layers\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEST_OUTPUT_DIR = \"cached_test_data\"\n",
    "os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "saes = get_saes(device, total_layers=26)\n",
    "feature_bidict = bidict()\n",
    "global_idx = 0\n",
    "for layer_idx, sae in enumerate(saes):\n",
    "    for feature_idx in range(sae.cfg.d_sae):\n",
    "        feature_bidict[(layer_idx, feature_idx)] = global_idx\n",
    "        global_idx += 1\n",
    "total_features = len(feature_bidict)\n",
    "print(f\"Created mapping for {total_features} features across {len(saes)} layers\")\n",
    "\n",
    "# Build tree\n",
    "MMLU_test_split = load_dataset(\"cais/mmlu\", \"all\", split='test')\n",
    "test_feature_vectors, test_correctness_labels = load_or_create_data(model, saes, feature_bidict, MMLU_test_split, TEST_OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
