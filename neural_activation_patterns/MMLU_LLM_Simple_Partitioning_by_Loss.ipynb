{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e279e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data. Loading from 'cached_test_data_losses'...\n",
      "output_logits set to None\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "1\n",
      "0.0025628693739352\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from bidict import bidict\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_saes(device, total_layers=26):\n",
    "    \"\"\"Loads a series of Sparse Autoencoders (SAEs) for specified layers\"\"\"\n",
    "    saes = []\n",
    "    print(f\"Loading {total_layers} SAEs...\")\n",
    "    for layer in range(total_layers):\n",
    "        sae, _, _ = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "            device=device\n",
    "        )\n",
    "        saes.append(sae)\n",
    "        print(f\"Layer {layer} loaded.\")\n",
    "    return saes\n",
    "\n",
    "def get_all_mmlu_questions(dataset):\n",
    "    \"\"\"Returns all questions from the MMLU dataset\"\"\"\n",
    "    all_questions = []\n",
    "    for i in range(len(dataset)):\n",
    "        data_point = dataset[i]\n",
    "        question = data_point['question']\n",
    "        choices = data_point['choices']\n",
    "        instruction = \"The following is a multiple choice question. Output only a single token corresponding to the right answer (ie: A) \\n\"\n",
    "        formatted_question = instruction + f\" Question: {question}\\n\"\n",
    "        choice_labels = ['A', 'B', 'C', 'D']\n",
    "        for j, choice in enumerate(choices):\n",
    "            formatted_question += f\"{choice_labels[j]}) {choice}\\n\"\n",
    "        formatted_question += \"Answer: \"\n",
    "        all_questions.append({\n",
    "            'text': formatted_question,\n",
    "            'subject': data_point['subject'],\n",
    "            'answer': data_point['answer'],\n",
    "            'choices': data_point['choices'],\n",
    "            'raw_question': question\n",
    "        })\n",
    "    return all_questions\n",
    "        \n",
    "def process_question(model, saes, question, feature_bidict, sample_idx=None):\n",
    "    \"\"\"\n",
    "    Performs a forward pass, extracts features and loss for a single MMLU question\n",
    "    \"\"\"\n",
    "    question_text = question['text']\n",
    "    correct_answer_idx = question['answer']\n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    total_features = len(feature_bidict)\n",
    "    feature_vector = np.zeros(total_features, dtype=np.byte)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tokens = model.to_tokens(question_text, prepend_bos=True).to(model.cfg.device)\n",
    "        logits, cache = model.run_with_cache(input_tokens)\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        choice_token_ids = [model.to_tokens(label, prepend_bos=False)[0, 0].item() for label in choice_labels]\n",
    "        choice_logits = last_token_logits[choice_token_ids]\n",
    "        output_logits = choice_logits.cpu().numpy()\n",
    "        loss = F.cross_entropy(choice_logits.unsqueeze(0), torch.tensor([correct_answer_idx]).to(model.cfg.device)).item()\n",
    "        predicted_choice_idx = torch.argmax(choice_logits).item()\n",
    "        is_correct = predicted_choice_idx == correct_answer_idx\n",
    "\n",
    "        print(f\"\\n--- Sample {sample_idx} ---\")\n",
    "        print(f\"Subject: {question['subject']}\")\n",
    "        print(f\"Question: {question['raw_question']}\")\n",
    "        print(\"Choices:\")\n",
    "        for i, choice in enumerate(question['choices']):\n",
    "            print(f\"  {choice_labels[i]}) {choice}\")\n",
    "        print(f\"Correct Answer: {choice_labels[correct_answer_idx]} ({question['choices'][correct_answer_idx]})\")\n",
    "        print(f\"Model's Answer: {choice_labels[predicted_choice_idx]} ({question['choices'][predicted_choice_idx]})\")\n",
    "        print(f\"Result: {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "        for layer_idx, sae in enumerate(saes):\n",
    "            final_token_activations = cache[sae.cfg.hook_name][0, -1, :].unsqueeze(0)\n",
    "            feature_acts = sae.encode(final_token_activations)\n",
    "            active_indices = torch.where(feature_acts > 0)[1].cpu().tolist()\n",
    "            for feature_idx in active_indices:\n",
    "                global_feature_idx = feature_bidict.get((layer_idx, feature_idx))\n",
    "                if global_feature_idx is not None:\n",
    "                    feature_vector[global_feature_idx] = 1\n",
    "\n",
    "    return feature_vector, is_correct, loss, output_logits\n",
    "        \n",
    "def extract_features_and_correctness(model, saes, questions, feature_bidict, output_dir=None):\n",
    "    \"\"\"\n",
    "    Processes questions to get features and correctness, with an option to save data\n",
    "    \"\"\"\n",
    "    all_feature_vectors = []\n",
    "    all_correctness_labels = []\n",
    "    all_losses = []\n",
    "    all_output_logits = []\n",
    "    for i, question in enumerate(questions):\n",
    "        feature_vector, is_correct, loss, output_logits = process_question(\n",
    "            model, saes, question, feature_bidict, sample_idx=i+1\n",
    "        )\n",
    "        all_feature_vectors.append(feature_vector)\n",
    "        all_correctness_labels.append(is_correct)\n",
    "        all_losses.append(loss)\n",
    "        all_output_logits.append(output_logits)\n",
    "\n",
    "    if output_dir:\n",
    "        features_np = np.array(all_feature_vectors, dtype=np.byte)\n",
    "        correctness_np = np.array(all_correctness_labels)\n",
    "        losses_np = np.array(all_losses)\n",
    "        output_logits_np =  np.array(all_output_logits)\n",
    "        feature_path = os.path.join(output_dir, f\"features.npy\")\n",
    "        correctness_path = os.path.join(output_dir, f\"correctness.npy\")\n",
    "        loss_path = os.path.join(output_dir, f\"losses.npy\")\n",
    "        output_logits_path = os.path.join(output_dir, f\"output_logits.npy\")\n",
    "        np.save(feature_path, features_np)\n",
    "        np.save(correctness_path, correctness_np)\n",
    "        np.save(loss_path, losses_np)\n",
    "        np.save(output_logits_path, output_logits_np)\n",
    "\n",
    "    return np.array(all_feature_vectors, dtype=np.byte), np.array(all_correctness_labels), np.array(all_losses), np.array(all_output_logits)\n",
    "\n",
    "def load_or_create_data(model, saes, feature_bidict, mmlu_dataset, output_dir):\n",
    "    feature_path = os.path.join(output_dir, \"features.npy\")\n",
    "    correctness_path = os.path.join(output_dir, \"correctness.npy\")\n",
    "    loss_path = os.path.join(output_dir, \"losses.npy\")\n",
    "    output_logits_path = os.path.join(output_dir, \"output_logits.npy\")\n",
    "    neurons_activations_path = os.path.join(output_dir, \"oneurons_activations_path.npy\")\n",
    "    if os.path.exists(feature_path) and os.path.exists(correctness_path) and os.path.exists(loss_path): #and os.path.exists(output_logits_path):\n",
    "        print(f\"Found cached data. Loading from '{output_dir}'...\")\n",
    "        feature_vectors = np.load(feature_path)\n",
    "        correctness_labels = np.load(correctness_path)\n",
    "        losses = np.load(loss_path)\n",
    "        try: \n",
    "            output_logits = np.load(output_logits_path)\n",
    "        except:\n",
    "            print(\"output_logits set to None\")\n",
    "            output_logits = None\n",
    "    else:\n",
    "        print(f\"No cached data found in '{output_dir}'. Generating data from scratch...\")\n",
    "        questions = get_all_mmlu_questions(mmlu_dataset)\n",
    "        feature_vectors, correctness_labels, losses, output_logits = extract_features_and_correctness(\n",
    "            model, saes, questions, feature_bidict,\n",
    "            output_dir=output_dir)\n",
    "    return feature_vectors, correctness_labels, losses, output_logits\n",
    "\n",
    "\n",
    "# Init all variables and load SAEs for all layers\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# TEST_OUTPUT_DIR = \"cached_test_data_losses\"#\"cached_test_data_correctness_losses_logits\"\n",
    "# os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n",
    "# model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "# saes = get_saes(device, total_layers=26)\n",
    "# feature_bidict = bidict()\n",
    "# global_idx = 0\n",
    "# for layer_idx, sae in enumerate(saes):\n",
    "#     for feature_idx in range(sae.cfg.d_sae):\n",
    "#         feature_bidict[(layer_idx, feature_idx)] = global_idx\n",
    "#         global_idx += 1\n",
    "# total_features = len(feature_bidict)\n",
    "# print(f\"Created mapping for {total_features} features across {len(saes)} layers\")\n",
    "# TEST_OUTPUT_DIR = \"cached_test_data_losses\"\n",
    "\n",
    "# Build tree\n",
    "MMLU_test_split = load_dataset(\"cais/mmlu\", \"all\", split='test')\n",
    "test_feature_vectors, test_correctness_labels, test_losses, test_output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_test_split, TEST_OUTPUT_DIR)\n",
    "print(test_feature_vectors)\n",
    "print(np.max(test_feature_vectors))\n",
    "print(np.mean(test_feature_vectors))\n",
    "X_test, X_stop_test, y_test, y_stop_test = train_test_split(\n",
    "    test_feature_vectors, \n",
    "    test_losses, \n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "stop_test_data = lgb.Dataset(X_stop_test, label=y_stop_test, reference=test_data)\n",
    "print(f\"\\nExtracted {len(test_feature_vectors)} samples for tree construction. Building NAP tree...\")\n",
    "start_time = time.time()\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': -1,\n",
    "    'num_leaves': 2048,\n",
    "    'min_data_in_leaf': 3,\n",
    "    'min_sum_hessian_in_leaf': 1e-4,\n",
    "    'feature_fraction': 1.0,\n",
    "    'feature_fraction_bynode': 1.0,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'learning_rate': 0.01,\n",
    "    'lambda_l1': 0.05,\n",
    "    'lambda_l2': 0.05,\n",
    "    'num_threads': -1,\n",
    "    'force_row_wise': True,\n",
    "    'verbosity': 1,\n",
    "    'seed': 42,\n",
    "    'deterministic': True\n",
    "}\n",
    "model_lgb = lgb.train(\n",
    "    params, \n",
    "    test_data, \n",
    "    num_boost_round=2000,\n",
    "    valid_sets=[stop_test_data],\n",
    "    valid_names=['stop_test_data'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n",
    "print(f\"LightGBM model built in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25d451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data. Loading from 'cached_validation_data_correctness_losses_logits'...\n",
      "Coverage of confident predictions: 0.019 (29/1531 validation samples)\n",
      "Accuracy on confident predictions: 0.9655\n",
      "Model's true validation accuracy: 0.4161\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_OUTPUT_DIR = \"cached_validation_data_activations_correctness_losses_logits\"\n",
    "os.makedirs(VALIDATION_OUTPUT_DIR, exist_ok=True)\n",
    "MMLU_validation_split = load_dataset(\"cais/mmlu\", \"all\", split='validation')\n",
    "validation_neuron_activations, validation_feature_vectors, validation_correctness_labels, validation_losses, output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_validation_split, VALIDATION_OUTPUT_DIR)\n",
    "validation_loss_predictions = model_lgb.predict(validation_feature_vectors)\n",
    "low_loss_threshold = 0.651\n",
    "confident_mask = validation_loss_predictions < low_loss_threshold\n",
    "confident_correctness_labels = validation_correctness_labels[confident_mask]\n",
    "confident_accuracy = np.mean(confident_correctness_labels)\n",
    "\n",
    "print(f\"Coverage of confident predictions: {np.mean(confident_mask):.3f} ({np.sum(confident_mask)}/{len(validation_feature_vectors)} validation samples)\")\n",
    "print(f\"Accuracy on confident predictions: {np.mean(confident_correctness_labels):.4f}\")\n",
    "print(f\"Model's true validation accuracy: {np.sum(validation_correctness_labels) / len(validation_correctness_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949a5e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.max(probs).item()\n\u001b[32m      5\u001b[39m VALIDATION_OUTPUT_DIR = \u001b[33m\"\u001b[39m\u001b[33mcached_validation_data_correctness_losses_logits\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mos\u001b[49m.makedirs(VALIDATION_OUTPUT_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m MMLU_validation_split = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mcais/mmlu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m'\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m validation_feature_vectors, validation_correctness_labels, validation_losses, output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_validation_split, VALIDATION_OUTPUT_DIR)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def softmax_max_probability(logits):\n",
    "    probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    return torch.max(probs).item()\n",
    "\n",
    "VALIDATION_OUTPUT_DIR = \"cached_validation_data_correctness_losses_logits\"\n",
    "os.makedirs(VALIDATION_OUTPUT_DIR, exist_ok=True)\n",
    "MMLU_validation_split = load_dataset(\"cais/mmlu\", \"all\", split='validation')\n",
    "validation_feature_vectors, validation_correctness_labels, validation_losses, output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_validation_split, VALIDATION_OUTPUT_DIR)\n",
    "confident_validation_outputs_mask = np.array([softmax_max_probability(output_logit) > 0.992 for output_logit in output_logits])\n",
    "confident_outputs_validation_labels = validation_correctness_labels[confident_validation_outputs_mask]\n",
    "validation_outputs_coverage = np.mean(confident_validation_outputs_mask)\n",
    "print(f\"Baseline coverage of confident NAPs: {validation_outputs_coverage:.3f}\")\n",
    "print(f\"Baseline Accuracy on confident predictions (how often model gets answer right): {np.sum(confident_outputs_validation_labels) / len(confident_outputs_validation_labels):.4f}\")\n",
    "print(f\"Baseline Model's true test accuracy (how often model gets answer right): {np.sum(validation_correctness_labels) / len(validation_correctness_labels):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estimating_nns.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
