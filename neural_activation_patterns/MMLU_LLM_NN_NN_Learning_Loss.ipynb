{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3f2c076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data. Loading from 'cached_test_data_losses'...\n",
      "output_logits set to None\n",
      "Epoch 1/100, Average Loss: 640.7592\n",
      "Epoch 2/100, Average Loss: 1.3137\n",
      "Epoch 3/100, Average Loss: 1.1803\n",
      "Epoch 4/100, Average Loss: 1.0212\n",
      "Epoch 5/100, Average Loss: 0.8593\n",
      "Epoch 6/100, Average Loss: 0.7336\n",
      "Epoch 7/100, Average Loss: 0.6015\n",
      "Epoch 8/100, Average Loss: 0.5211\n",
      "Epoch 9/100, Average Loss: 0.5133\n",
      "Epoch 10/100, Average Loss: 0.3972\n",
      "Epoch 11/100, Average Loss: 0.3186\n",
      "Epoch 12/100, Average Loss: 0.2774\n",
      "Epoch 13/100, Average Loss: 0.2565\n",
      "Epoch 14/100, Average Loss: 0.2548\n",
      "Epoch 15/100, Average Loss: 0.2061\n",
      "Epoch 16/100, Average Loss: 0.1709\n",
      "Epoch 17/100, Average Loss: 0.1527\n",
      "Epoch 18/100, Average Loss: 0.1399\n",
      "Epoch 19/100, Average Loss: 0.1258\n",
      "Epoch 20/100, Average Loss: 0.1187\n",
      "Epoch 21/100, Average Loss: 0.1160\n",
      "Epoch 22/100, Average Loss: 0.1039\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 208\u001b[39m\n\u001b[32m    206\u001b[39m test_feature_vectors = selector.fit_transform(test_feature_vectors, test_losses)\n\u001b[32m    207\u001b[39m supervisor = SupervisorNN(test_feature_vectors.shape[\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[43mtrain_supervisor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msupervisor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_feature_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mtrain_supervisor\u001b[39m\u001b[34m(supervisor, test_feature_vectors, test_losses, epochs, batch_size)\u001b[39m\n\u001b[32m    180\u001b[39m     loss = criterion(output, batch_y)\n\u001b[32m    181\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     total_loss += loss.item()\n\u001b[32m    184\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(data_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/torch/optim/adam.py:527\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    525\u001b[39m         denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch.is_complex(params[i]):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "from bidict import bidict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def get_saes(device, total_layers=26):\n",
    "    \"\"\"Loads a series of Sparse Autoencoders (SAEs) for specified layers\"\"\"\n",
    "    saes = []\n",
    "    print(f\"Loading {total_layers} SAEs...\")\n",
    "    for layer in range(total_layers):\n",
    "        sae, _, _ = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "            device=device\n",
    "        )\n",
    "        saes.append(sae)\n",
    "        print(f\"Layer {layer} loaded.\")\n",
    "    return saes\n",
    "\n",
    "def get_all_mmlu_questions(dataset):\n",
    "    \"\"\"Returns all questions from the MMLU dataset\"\"\"\n",
    "    all_questions = []\n",
    "    for i in range(len(dataset)):\n",
    "        data_point = dataset[i]\n",
    "        question = data_point['question']\n",
    "        choices = data_point['choices']\n",
    "        instruction = \"The following is a multiple choice question. Output only a single token corresponding to the right answer (ie: A) \\n\"\n",
    "        formatted_question = instruction + f\" Question: {question}\\n\"\n",
    "        choice_labels = ['A', 'B', 'C', 'D']\n",
    "        for j, choice in enumerate(choices):\n",
    "            formatted_question += f\"{choice_labels[j]}) {choice}\\n\"\n",
    "        formatted_question += \"Answer: \"\n",
    "        all_questions.append({\n",
    "            'text': formatted_question,\n",
    "            'subject': data_point['subject'],\n",
    "            'answer': data_point['answer'],\n",
    "            'choices': data_point['choices'],\n",
    "            'raw_question': question\n",
    "        })\n",
    "    return all_questions\n",
    "        \n",
    "def process_question(model, saes, question, feature_bidict, sample_idx=None):\n",
    "    \"\"\"\n",
    "    Performs a forward pass, extracts features and loss for a single MMLU question\n",
    "    \"\"\"\n",
    "    question_text = question['text']\n",
    "    correct_answer_idx = question['answer']\n",
    "    choice_labels = ['A', 'B', 'C', 'D']\n",
    "    total_features = len(feature_bidict)\n",
    "    feature_vector = np.zeros(total_features, dtype=np.byte)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_tokens = model.to_tokens(question_text, prepend_bos=True).to(model.cfg.device)\n",
    "        logits, cache = model.run_with_cache(input_tokens)\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        choice_token_ids = [model.to_tokens(label, prepend_bos=False)[0, 0].item() for label in choice_labels]\n",
    "        choice_logits = last_token_logits[choice_token_ids]\n",
    "        output_logits = choice_logits.cpu().numpy()\n",
    "        loss = F.cross_entropy(choice_logits.unsqueeze(0), torch.tensor([correct_answer_idx]).to(model.cfg.device)).item()\n",
    "        predicted_choice_idx = torch.argmax(choice_logits).item()\n",
    "        is_correct = predicted_choice_idx == correct_answer_idx\n",
    "\n",
    "        print(f\"\\n--- Sample {sample_idx} ---\")\n",
    "        print(f\"Subject: {question['subject']}\")\n",
    "        print(f\"Question: {question['raw_question']}\")\n",
    "        print(\"Choices:\")\n",
    "        for i, choice in enumerate(question['choices']):\n",
    "            print(f\"  {choice_labels[i]}) {choice}\")\n",
    "        print(f\"Correct Answer: {choice_labels[correct_answer_idx]} ({question['choices'][correct_answer_idx]})\")\n",
    "        print(f\"Model's Answer: {choice_labels[predicted_choice_idx]} ({question['choices'][predicted_choice_idx]})\")\n",
    "        print(f\"Result: {'✓ CORRECT' if is_correct else '✗ INCORRECT'}\")\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "        for layer_idx, sae in enumerate(saes):\n",
    "            final_token_activations = cache[sae.cfg.hook_name][0, -1, :].unsqueeze(0)\n",
    "            feature_acts = sae.encode(final_token_activations)\n",
    "            active_indices = torch.where(feature_acts > 0)[1].cpu().tolist()\n",
    "            for feature_idx in active_indices:\n",
    "                global_feature_idx = feature_bidict.get((layer_idx, feature_idx))\n",
    "                if global_feature_idx is not None:\n",
    "                    feature_vector[global_feature_idx] = 1\n",
    "\n",
    "    return feature_vector, is_correct, loss, output_logits\n",
    "        \n",
    "def extract_features_and_correctness(model, saes, questions, feature_bidict, output_dir=None):\n",
    "    \"\"\"\n",
    "    Processes questions to get features and correctness, with an option to save data\n",
    "    \"\"\"\n",
    "    all_feature_vectors = []\n",
    "    all_correctness_labels = []\n",
    "    all_losses = []\n",
    "    all_output_logits = []\n",
    "    for i, question in enumerate(questions):\n",
    "        feature_vector, is_correct, loss, output_logits = process_question(\n",
    "            model, saes, question, feature_bidict, sample_idx=i+1\n",
    "        )\n",
    "        all_feature_vectors.append(feature_vector)\n",
    "        all_correctness_labels.append(is_correct)\n",
    "        all_losses.append(loss)\n",
    "        all_output_logits.append(output_logits)\n",
    "\n",
    "    if output_dir:\n",
    "        features_np = np.array(all_feature_vectors, dtype=np.byte)\n",
    "        correctness_np = np.array(all_correctness_labels)\n",
    "        losses_np = np.array(all_losses)\n",
    "        output_logits_np =  np.array(all_output_logits)\n",
    "        feature_path = os.path.join(output_dir, f\"features.npy\")\n",
    "        correctness_path = os.path.join(output_dir, f\"correctness.npy\")\n",
    "        loss_path = os.path.join(output_dir, f\"losses.npy\")\n",
    "        output_logits_path = os.path.join(output_dir, f\"output_logits.npy\")\n",
    "        np.save(feature_path, features_np)\n",
    "        np.save(correctness_path, correctness_np)\n",
    "        np.save(loss_path, losses_np)\n",
    "        np.save(output_logits_path, output_logits_np)\n",
    "\n",
    "    return np.array(all_feature_vectors, dtype=np.byte), np.array(all_correctness_labels), np.array(all_losses), np.array(all_output_logits)\n",
    "\n",
    "def load_or_create_data(model, saes, feature_bidict, mmlu_dataset, output_dir):\n",
    "    feature_path = os.path.join(output_dir, \"features.npy\")\n",
    "    correctness_path = os.path.join(output_dir, \"correctness.npy\")\n",
    "    loss_path = os.path.join(output_dir, \"losses.npy\")\n",
    "    output_logits_path = os.path.join(output_dir, \"output_logits.npy\")\n",
    "    if os.path.exists(feature_path) and os.path.exists(correctness_path) and os.path.exists(loss_path): #and os.path.exists(output_logits_path):\n",
    "        print(f\"Found cached data. Loading from '{output_dir}'...\")\n",
    "        feature_vectors = np.load(feature_path)\n",
    "        correctness_labels = np.load(correctness_path)\n",
    "        losses = np.load(loss_path)\n",
    "        try: \n",
    "            output_logits = np.load(output_logits_path)\n",
    "        except:\n",
    "            print(\"output_logits set to None\")\n",
    "            output_logits = None\n",
    "    else:\n",
    "        print(f\"No cached data found in '{output_dir}'. Generating data from scratch...\")\n",
    "        questions = get_all_mmlu_questions(mmlu_dataset)\n",
    "        feature_vectors, correctness_labels, losses, output_logits = extract_features_and_correctness(\n",
    "            model, saes, questions, feature_bidict,\n",
    "            output_dir=output_dir)\n",
    "    return feature_vectors, correctness_labels, losses, output_logits\n",
    "\n",
    "class SupervisorNN(nn.Module):\n",
    "    def __init__(self, activation_len):\n",
    "        super(SupervisorNN, self).__init__()\n",
    "        self.activation_len = activation_len\n",
    "        self.fc1 = nn.Linear(activation_len, activation_len)\n",
    "        self.fc2 = nn.Linear(activation_len, activation_len)\n",
    "        self.fc3 = nn.Linear(activation_len, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def train_supervisor(supervisor, test_feature_vectors, test_losses, epochs=10, batch_size=256):\n",
    "    X = torch.FloatTensor(test_feature_vectors)\n",
    "    y = torch.FloatTensor(test_losses).unsqueeze(1)  # Add dimension for regression\n",
    "\n",
    "    dataset = TensorDataset(X, y)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    supervisor.to(device)\n",
    "    supervisor.train()\n",
    "    optimizer = torch.optim.Adam(supervisor.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()  # Use MSE loss for regression\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = supervisor(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(data_loader):.4f}')\n",
    "\n",
    "\n",
    "# Init all variables and load SAEs for all layers\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# TEST_OUTPUT_DIR = \"cached_test_data_losses\"#\"cached_test_data_correctness_losses_logits\"\n",
    "# os.makedirs(TEST_OUTPUT_DIR, exist_ok=True)\n",
    "# model = HookedTransformer.from_pretrained(\"gemma-2-2b\", device=device)\n",
    "# saes = get_saes(device, total_layers=26)\n",
    "# feature_bidict = bidict()\n",
    "# global_idx = 0\n",
    "# for layer_idx, sae in enumerate(saes):\n",
    "#     for feature_idx in range(sae.cfg.d_sae):\n",
    "#         feature_bidict[(layer_idx, feature_idx)] = global_idx\n",
    "#         global_idx += 1\n",
    "# total_features = len(feature_bidict)\n",
    "# print(f\"Created mapping for {total_features} features across {len(saes)} layers\")\n",
    "\n",
    "# Cut down to 50000 top features and train NN \n",
    "MMLU_test_split = load_dataset(\"cais/mmlu\", \"all\", split='test')\n",
    "test_feature_vectors, test_correctness_labels, test_losses, test_output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_test_split, TEST_OUTPUT_DIR)\n",
    "selector = SelectKBest(score_func=f_regression, k=15000)\n",
    "test_feature_vectors = selector.fit_transform(test_feature_vectors, test_losses)\n",
    "supervisor = SupervisorNN(test_feature_vectors.shape[1])\n",
    "train_supervisor(supervisor, test_feature_vectors, test_losses, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9076d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since cais/mmlu couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since cais/mmlu couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'all' at /Users/noahschwartz/.cache/huggingface/datasets/cais___mmlu/all/0.0.0/c30699e8356da336a370243923dbaf21066bb9fe (last modified on Mon Jul 14 09:52:53 2025).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'all' at /Users/noahschwartz/.cache/huggingface/datasets/cais___mmlu/all/0.0.0/c30699e8356da336a370243923dbaf21066bb9fe (last modified on Mon Jul 14 09:52:53 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data. Loading from 'cached_validation_data_correctness_losses_logits'...\n",
      "Coverage of confident predictions: 0.036 (55/1531 validation samples)\n",
      "Accuracy on confident predictions: 0.8182\n",
      "Model's true validation accuracy: 0.4161\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_OUTPUT_DIR = \"cached_validation_data_correctness_losses_logits\"\n",
    "os.makedirs(VALIDATION_OUTPUT_DIR, exist_ok=True)\n",
    "MMLU_validation_split = load_dataset(\"cais/mmlu\", \"all\", split='validation')\n",
    "validation_feature_vectors, validation_correctness_labels, validation_losses, output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_validation_split, VALIDATION_OUTPUT_DIR)\n",
    "validation_feature_vectors = selector.transform(validation_feature_vectors)\n",
    "\n",
    "supervisor.eval()\n",
    "with torch.no_grad():\n",
    "    validation_features_tensor = torch.FloatTensor(validation_feature_vectors).to(device)\n",
    "    validation_loss_predictions = supervisor(validation_features_tensor).cpu().numpy().flatten()\n",
    "\n",
    "low_loss_threshold = 0.\n",
    "confident_mask = validation_loss_predictions < low_loss_threshold\n",
    "confident_correctness_labels = validation_correctness_labels[confident_mask]\n",
    "confident_accuracy = np.mean(confident_correctness_labels)\n",
    "\n",
    "print(f\"Coverage of confident predictions: {np.mean(confident_mask):.3f} ({np.sum(confident_mask)}/{len(validation_feature_vectors)} validation samples)\")\n",
    "print(f\"Accuracy on confident predictions: {np.mean(confident_correctness_labels):.4f}\")\n",
    "print(f\"Model's true validation accuracy: {np.sum(validation_correctness_labels) / len(validation_correctness_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6326c2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of confident predictions: 0.005 (7/1531 validation samples)\n",
      "Accuracy on confident predictions: 0.8571\n",
      "Model's true validation accuracy: 0.4161\n"
     ]
    }
   ],
   "source": [
    "low_loss_threshold = 0.0000001\n",
    "confident_mask = validation_loss_predictions < low_loss_threshold\n",
    "confident_correctness_labels = validation_correctness_labels[confident_mask]\n",
    "confident_accuracy = np.mean(confident_correctness_labels)\n",
    "\n",
    "print(f\"Coverage of confident predictions: {np.mean(confident_mask):.3f} ({np.sum(confident_mask)}/{len(validation_feature_vectors)} validation samples)\")\n",
    "print(f\"Accuracy on confident predictions: {np.mean(confident_correctness_labels):.4f}\")\n",
    "print(f\"Model's true validation accuracy: {np.sum(validation_correctness_labels) / len(validation_correctness_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a4937bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data. Loading from 'cached_validation_data_correctness_losses_logits'...\n",
      "Baseline coverage of confident NAPs: 1.000\n",
      "Baseline Accuracy on confident predictions (how often model gets answer right): 0.4161\n",
      "Baseline Model's true test accuracy (how often model gets answer right): 0.4161\n"
     ]
    }
   ],
   "source": [
    "def softmax_max_probability(logits):\n",
    "    probs = F.softmax(torch.tensor(logits), dim=-1)\n",
    "    return torch.max(probs).item()\n",
    "\n",
    "VALIDATION_OUTPUT_DIR = \"cached_validation_data_correctness_losses_logits\"\n",
    "os.makedirs(VALIDATION_OUTPUT_DIR, exist_ok=True)\n",
    "MMLU_validation_split = load_dataset(\"cais/mmlu\", \"all\", split='validation')\n",
    "validation_feature_vectors, validation_correctness_labels, validation_losses, output_logits = load_or_create_data(model, saes, feature_bidict, MMLU_validation_split, VALIDATION_OUTPUT_DIR)\n",
    "confident_validation_outputs_mask = np.array([softmax_max_probability(output_logit) > 0. for output_logit in output_logits])\n",
    "confident_outputs_validation_labels = validation_correctness_labels[confident_validation_outputs_mask]\n",
    "validation_outputs_coverage = np.mean(confident_validation_outputs_mask)\n",
    "print(f\"Baseline coverage of confident NAPs: {validation_outputs_coverage:.3f}\")\n",
    "print(f\"Baseline Accuracy on confident predictions (how often model gets answer right): {np.sum(confident_outputs_validation_labels) / len(confident_outputs_validation_labels):.4f}\")\n",
    "print(f\"Baseline Model's true test accuracy (how often model gets answer right): {np.sum(validation_correctness_labels) / len(validation_correctness_labels):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
