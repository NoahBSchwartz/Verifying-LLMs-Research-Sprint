{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b6ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "class FactorGaussian:\n",
    "    def __init__(self, mean, L_factor, diagonal):\n",
    "        \"\"\"Î£ = L @ L.T + diag(diagonal)\"\"\"\n",
    "        self.mean = mean\n",
    "        self.L = L_factor\n",
    "        self.diagonal = diagonal\n",
    "\n",
    "    def covariance(self):\n",
    "        stable_diagonal = self.diagonal + 1e-8\n",
    "        return self.L @ self.L.T + th.diag(stable_diagonal)\n",
    "\n",
    "    def get_diagonal(self):\n",
    "        l_diag = th.sum(self.L * self.L, dim=1)\n",
    "        return l_diag + self.diagonal\n",
    "\n",
    "def propagate_by_sampling(input_distribution, layer_block, mask=None, num_samples=1000, device='cpu'):\n",
    "    input_mean = input_distribution.mean\n",
    "    input_cov = input_distribution.covariance()\n",
    "    input_dim = input_mean.shape[0]\n",
    "    \n",
    "    print(f\"    - Propagating through layer: {layer_block.__class__.__name__}\")\n",
    "    print(f\"    - Input mean shape: {input_mean.shape}\")\n",
    "    print(f\"    - Input covariance shape: {input_cov.shape}\")\n",
    "    print(f\"    - Number of samples for propagation: {num_samples}\")\n",
    "    \n",
    "    epsilon = 1e-6 * th.eye(input_dim, device=input_mean.device)\n",
    "    try:\n",
    "        input_dist_sampler = MultivariateNormal(loc=input_mean, covariance_matrix=input_cov + epsilon)\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"    - WARNING: Covariance matrix not positive definite. Adding larger epsilon.\")\n",
    "        epsilon = 1e-4 * torch.eye(input_dim, device=input_mean.device)\n",
    "        input_dist_sampler = MultivariateNormal(loc=input_mean, covariance_matrix=input_cov + epsilon)\n",
    "\n",
    "    samples = input_dist_sampler.sample(th.Size([num_samples]))\n",
    "    print(f\"    - Drawn samples shape: {samples.shape}\")\n",
    "    \n",
    "    samples_reshaped = samples.unsqueeze(1)\n",
    "\n",
    "    with th.no_grad():\n",
    "        output_samples_reshaped = layer_block(samples_reshaped.to(device))\n",
    "\n",
    "    output_samples = output_samples_reshaped.squeeze(1)\n",
    "    print(f\"    - Output samples shape after layer block: {output_samples.shape}\")\n",
    "\n",
    "    output_mean = th.mean(output_samples, dim=0)\n",
    "    mean_subtracted_output = output_samples - output_mean\n",
    "    full_output_covariance = (1 / (num_samples - 1)) * mean_subtracted_output.T @ mean_subtracted_output\n",
    "    output_dim = output_mean.shape[0]\n",
    "    \n",
    "    print(f\"    - Output mean shape: {output_mean.shape}\")\n",
    "    print(f\"    - Full output covariance shape: {full_output_covariance.shape}\")\n",
    "\n",
    "    if mask is not None and len(mask) > 0:\n",
    "        print(f\"    - Applying mask with {len(mask)} entries.\")\n",
    "        masked_cov = th.zeros_like(full_output_covariance)\n",
    "        valid_mask_entries = [(r, c) for r, c in mask if r < output_dim and c < output_dim]\n",
    "        rows, cols = zip(*valid_mask_entries)\n",
    "        masked_cov[rows, cols] = full_output_covariance[rows, cols]\n",
    "    else:\n",
    "        print(\"    - No mask provided, using diagonal covariance.\")\n",
    "        masked_cov = th.diag(th.diag(full_output_covariance))\n",
    "        \n",
    "    output_diag_variances = th.diag(full_output_covariance)\n",
    "    off_diag_cov = masked_cov - th.diag(th.diag(masked_cov))\n",
    "\n",
    "    try:\n",
    "        eigvals, eigvecs = th.linalg.eigh(off_diag_cov)\n",
    "        eigvals_positive = th.clamp(eigvals, min=0)\n",
    "        sqrt_eigvals = th.sqrt(eigvals_positive)\n",
    "        tol = 1e-6\n",
    "        rank = th.sum(eigvals_positive > tol).item()\n",
    "        print(f\"    - Off-diagonal covariance matrix rank: {rank}\")\n",
    "        \n",
    "        if rank > 0:\n",
    "            output_L = eigvecs[:, -rank:] @ th.diag(sqrt_eigvals[-rank:])\n",
    "            l_diag_contribution = th.sum(output_L * output_L, dim=1)\n",
    "            output_diag_remainder = th.clamp(output_diag_variances - l_diag_contribution, min=0)\n",
    "        else:\n",
    "            output_L = th.zeros((output_dim, 1), device=output_mean.device)\n",
    "            output_diag_remainder = output_diag_variances\n",
    "            print(\"    - Rank is 0, using diagonal remainder for covariance.\")\n",
    "\n",
    "    except torch.linalg.LinAlgError:\n",
    "        print(\"    - WARNING: Eigendecomposition failed. Falling back to diagonal approximation.\")\n",
    "        output_L = th.zeros((output_dim, 1), device=output_mean.device)\n",
    "        output_diag_remainder = output_diag_variances\n",
    "\n",
    "    print(f\"    - L factor shape: {output_L.shape}\")\n",
    "    print(f\"    - Diagonal remainder shape: {output_diag_remainder.shape}\")\n",
    "\n",
    "    return FactorGaussian(mean=output_mean, L_factor=output_L, diagonal=output_diag_remainder)\n",
    "\n",
    "\n",
    "def gaussian_sampling_estimator(model, orig_dists: list[Discrete], target: int, *, n_samples: int, batch_size: int, n_off_diagonal_entries: int = 0, show_progress: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Run the initial samples through the first layer to create the initial distribution we'll draw from\n",
    "    initial_samples = th.stack([dist.sample((n_samples,)) for dist in orig_dists], dim=1)\n",
    "    with th.no_grad():\n",
    "        tok_emb = model.embed(initial_samples)\n",
    "        pos_emb = model.pos_embed(initial_samples)\n",
    "        initial_activations = tok_emb + pos_emb\n",
    "        flat_initial_activations = initial_activations.reshape(-1, model.cfg['d_model'])\n",
    "    mean = th.mean(flat_initial_activations, dim=0)\n",
    "    cov = th.cov(flat_initial_activations.T)\n",
    "    epsilon_diag = 1e-6 * th.eye(cov.shape[0], device=cov.device)\n",
    "    # Compute Cholesky decomposition for initial covariance (very helpful when we start adding off-diagonal entries)\n",
    "    L = th.linalg.cholesky(cov + epsilon_diag)\n",
    "    diag = th.zeros_like(mean) \n",
    "    current_dist = FactorGaussian(mean=mean, L_factor=L, diagonal=diag)\n",
    "\n",
    "    # Propagate with no masking\n",
    "    print(f\"MODEL BLOCKS: {model.blocks}\")\n",
    "    grouped_layers = model.blocks\n",
    "    masks = []\n",
    "    all_covariance_entries = []\n",
    "    temp_dist = current_dist\n",
    "    for layer_idx, layer_block in enumerate(grouped_layers):\n",
    "        output_dim = model.cfg['d_model']\n",
    "        print(f\"Output Dim: {output_dim}\")\n",
    "        full_mask = set((r, c) for r in range(output_dim) for c in range(output_dim))\n",
    "        temp_dist = propagate_by_sampling(temp_dist, layer_block, mask=full_mask, num_samples=10000, device=model.device)\n",
    "        full_cov = temp_dist.covariance()\n",
    "        for r in range(output_dim):\n",
    "            for c in range(r + 1, output_dim):\n",
    "                all_covariance_entries.append((abs(full_cov[r, c].item()), layer_idx, r, c))\n",
    "    all_covariance_entries.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Propagate with masking (just for fun)\n",
    "    for block in grouped_layers:\n",
    "        dim = model.cfg['d_model']\n",
    "        masks.append(set((i, i) for i in range(dim)))\n",
    "    for _, layer_idx, r, c in all_covariance_entries[:n_off_diagonal_entries]:\n",
    "        masks[layer_idx].add((r,c))\n",
    "        masks[layer_idx].add((c,r))\n",
    "    for i, layer_block in enumerate(grouped_layers):\n",
    "        current_dist = propagate_by_sampling(current_dist, layer_block, mask=masks[i], num_samples=batch_size, device=model.device)\n",
    "    # Propagate through final layer norm\n",
    "    final_ln_mask = set((i, i) for i in range(model.cfg['d_model']))\n",
    "    current_dist = propagate_by_sampling(current_dist, model.ln_final, mask=final_ln_mask, num_samples=batch_size, device=model.device)\n",
    "\n",
    "    # 3. Final probability estimation\n",
    "    final_mean = current_dist.mean\n",
    "    final_cov = current_dist.covariance()\n",
    "    final_dist_sampler = MultivariateNormal(loc=final_mean, covariance_matrix=final_cov + epsilon_diag)\n",
    "    final_act_samples = final_dist_sampler.sample(th.Size([n_samples]))\n",
    "    print(f\"  - Sampled final activations, shape: {final_act_samples.shape}\")\n",
    "    \n",
    "    # Get logits and probabilities\n",
    "    with th.no_grad():\n",
    "        logits = final_act_samples @ model.unembed.W_U\n",
    "        probs = th.softmax(logits, dim=-1)\n",
    "    \n",
    "    print(f\"  - Computed final logits and probabilities, shape: {probs.shape}\")\n",
    "\n",
    "    # Average probability of the target token\n",
    "    target_probs = probs[:, target]\n",
    "    estimated_prob = target_probs.mean().item()\n",
    "    print(f\"\\n--- Estimated Probability for target {target}: {estimated_prob:.6e} ---\")\n",
    "    \n",
    "    return estimated_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04332ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "estimating_nns.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
