{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70b8f7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 22.6110\n",
      "Epoch 20, Loss: 22.5852\n",
      "Epoch 40, Loss: 22.4629\n",
      "Epoch 60, Loss: 21.6043\n",
      "Epoch 80, Loss: 12.8311\n",
      "\n",
      "Model Architecture:\n",
      "DeepNeuralNet(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=4, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=4, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=4, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total possible covariance entries across all layers: 362\n",
      "Initial diagonal entries: 24\n",
      "Computing optimal ordering (this may take a while)...\n",
      "Linear(in_features=3, out_features=4, bias=True) 3\n",
      "ReLU() 4\n",
      "Linear(in_features=4, out_features=12, bias=True) 4\n",
      "ReLU() 12\n",
      "Linear(in_features=12, out_features=4, bias=True) 12\n",
      "ReLU() 4\n",
      "Linear(in_features=4, out_features=1, bias=True) 4\n",
      "  Step 20: Added (3, 1) to layer 2, KL = 0.284682\n",
      "  Step 40: Added (0, 11) to layer 4, KL = 0.283059\n",
      "  Step 60: Added (2, 5) to layer 3, KL = 0.283053\n",
      "  Step 80: Added (3, 4) to layer 3, KL = 0.280320\n",
      "  Step 100: Added (4, 6) to layer 3, KL = 0.280114\n",
      "  Step 120: Added (5, 8) to layer 4, KL = 0.216691\n",
      "  Step 140: Added (6, 9) to layer 3, KL = 0.147469\n",
      "  Step 160: Added (7, 10) to layer 4, KL = 0.106435\n",
      "  Step 180: Added (8, 10) to layer 3, KL = 0.105712\n",
      "  Step 200: Added (9, 8) to layer 4, KL = 0.074323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahschwartz/Desktop/Estimating_NNs/.venv/lib/python3.11/site-packages/numpy/linalg/_linalg.py:2430: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 220: Added (10, 9) to layer 4, KL = 0.069539\n",
      "  Step 240: Added (11, 9) to layer 3, KL = 0.066482\n",
      "  Step 260: Added (3, 0) to layer 6, KL = 0.063582\n",
      "  Step 280: Added (2, 0) to layer 4, KL = 0.063586\n",
      "  Step 300: Added (9, 4) to layer 4, KL = 0.063782\n",
      "No more beneficial or valid entries found after 318 additions. Terminating.\n",
      "Optimal ordering computed with 318 entries.\n",
      "Optimal ordering contains 318 entries\n",
      "Running 318 training steps\n",
      "Step 1: 6.63% filled, KL Div = 0.599239, Est. Exceedance = 0.000144/0.126254, Cov. Ratio = 0.126523, Next: (0, 1, 0)\n",
      "Step 200: 61.88% filled, KL Div = 0.106682, Est. Exceedance = 0.059274/0.126254, Cov. Ratio = 0.494401, Next: (3, 9, 10)\n",
      "Step 201: 61.88% filled, KL Div = 0.103791, Est. Exceedance = 0.060682/0.126254, Cov. Ratio = 0.500475, Next: (4, 9, 10)\n"
     ]
    },
    {
     "ename": "CholmodNotPositiveDefiniteError",
     "evalue": "/private/tmp/suite-sparse-20250505-6185-gw9cj5/SuiteSparse-7.10.3/CHOLMOD/Cholesky/t_cholmod_rowfac_worker.c:433: not positive definite (code 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCholmodNotPositiveDefiniteError\u001b[39m           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 414\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model.network):\n\u001b[32m    413\u001b[39m     output_mean, output_covariance = propagate_gaussian_analytically(current_mean, current_covariance, layer, masks[layer_idx])\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     propagated_layer_gaussian = \u001b[43mSparseGaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_covariance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m     propagated_gaussians.append(propagated_layer_gaussian)\n\u001b[32m    416\u001b[39m     current_mean = output_mean\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mSparseGaussian.__init__\u001b[39m\u001b[34m(self, mean, covariance_sparse)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mself\u001b[39m.mean = mean.cpu().numpy()\n\u001b[32m     42\u001b[39m \u001b[38;5;28mself\u001b[39m.covariance_sparse = covariance_sparse\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mself\u001b[39m.factor = \u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovariance_sparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msksparse/cholmod.pyx:1189\u001b[39m, in \u001b[36msksparse.cholmod.cholesky\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msksparse/cholmod.pyx:1217\u001b[39m, in \u001b[36msksparse.cholmod._cholesky\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msksparse/cholmod.pyx:604\u001b[39m, in \u001b[36msksparse.cholmod.Factor._cholesky_inplace\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msksparse/cholmod.pyx:599\u001b[39m, in \u001b[36msksparse.cholmod.Factor._cholesky_inplace\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msksparse/cholmod.pyx:387\u001b[39m, in \u001b[36msksparse.cholmod._error_handler\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCholmodNotPositiveDefiniteError\u001b[39m: /private/tmp/suite-sparse-20250505-6185-gw9cj5/SuiteSparse-7.10.3/CHOLMOD/Cholesky/t_cholmod_rowfac_worker.c:433: not positive definite (code 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, coo_matrix\n",
    "from sksparse.cholmod import cholesky\n",
    "import random\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('training_visualizations', exist_ok=True)\n",
    "EPSILON = 1e-9\n",
    "\n",
    "class DeepNeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(DeepNeuralNet, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Hidden layers with ReLU activations\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Last hidden to output (no activation on output layer)\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class SparseGaussian:\n",
    "    def __init__(self, mean, covariance_sparse):\n",
    "        self.mean = mean.cpu().numpy()\n",
    "        self.covariance_sparse = covariance_sparse\n",
    "        self.factor = cholesky(covariance_sparse)\n",
    "        \n",
    "    def sample(self, sample_shape):\n",
    "        if isinstance(sample_shape, tuple):\n",
    "            n_samples = sample_shape[0]\n",
    "        else:\n",
    "            n_samples = sample_shape\n",
    "        \n",
    "        z = np.random.randn(len(self.mean), n_samples)\n",
    "        samples = self.factor.solve_A(z)\n",
    "        samples = self.mean[:, np.newaxis] + samples\n",
    "        return torch.from_numpy(samples.T).float()\n",
    "    \n",
    "    def log_prob(self, value):\n",
    "        # Simplified log probability calculation\n",
    "        value_np = value.cpu().numpy()\n",
    "        centered = value_np - self.mean\n",
    "        return torch.tensor(-0.5 * np.sum(centered**2))\n",
    "\n",
    "def create_diagonal_mask(dimension):\n",
    "    mask = []\n",
    "    for i in range(dimension):\n",
    "        mask.append((i, i))\n",
    "    return mask\n",
    "\n",
    "def update_mask(mask, dimension):\n",
    "    n = dimension\n",
    "    if len(mask) >= n * n:\n",
    "        return mask\n",
    "    while True:\n",
    "        i, j = random.randint(0, n-1), random.randint(0, n-1)\n",
    "        if (i, j) not in mask:\n",
    "            mask.append((i, j))\n",
    "            break\n",
    "    return mask\n",
    "\n",
    "def standard_gaussian(x):\n",
    "    return (1./np.sqrt(2*np.pi)) * torch.exp(-x*x / 2.0)\n",
    "\n",
    "def gaussian_cdf(x):\n",
    "    return 0.5 * (1.0 + torch.erf(x * (1./np.sqrt(2))))\n",
    "\n",
    "def relu_covariance(input_mean, input_covariance, std_of_neuron_variances):\n",
    "    # Cov(ReLU(X_i), ReLU(X_j)) = E[ReLU(X_i) * ReLU(X_j)] - E[ReLU(X_i)] * E[ReLU(X_j)], Very hard bc ReLU clips negative vals\n",
    "    input_covariance = input_covariance.tocoo()\n",
    "    # Pre-compute values needed for all neurons (computed once, reused)\n",
    "    normalized_mean = input_mean / std_of_neuron_variances\n",
    "    gaussian_cdf_vals = gaussian_cdf(normalized_mean)\n",
    "    standard_gaussian_vals = standard_gaussian(normalized_mean)\n",
    "    rows, cols, data = [], [], []\n",
    "    for idx in range(len(input_covariance.data)):\n",
    "        i, j = input_covariance.row[idx], input_covariance.col[idx]\n",
    "        input_cov_ij = input_covariance.data[idx]\n",
    "        # Compute correlation coefficient matrix: correlations[i,j] = input_covariance[i,j] / (std_of_neuron_variances[i] * std_of_neuron_variances[j])\n",
    "        correlation_coefficient = input_cov_ij / (std_of_neuron_variances[i] * std_of_neuron_variances[j])\n",
    "        correlation_coefficient = torch.clamp(correlation_coefficient, -1/(1+EPSILON), 1/(1+EPSILON))  # Ensure valid correlations\n",
    "        # 4-term series expansion for ReLU covariance\n",
    "        t1 = correlation_coefficient * std_of_neuron_variances[i] * gaussian_cdf_vals[i] * \\\n",
    "             std_of_neuron_variances[j] * gaussian_cdf_vals[j]\n",
    "        t2 = (1./2) * correlation_coefficient**2 * std_of_neuron_variances[i] * standard_gaussian_vals[i] * \\\n",
    "             std_of_neuron_variances[j] * standard_gaussian_vals[j]\n",
    "        t3 = (1./6) * correlation_coefficient**3 * (-input_mean[i]) * standard_gaussian_vals[i] * \\\n",
    "             (-input_mean[j]) * standard_gaussian_vals[j]\n",
    "        t4 = (1./24) * correlation_coefficient**4 * std_of_neuron_variances[i] * \\\n",
    "             (normalized_mean[i]**2 - 1) * standard_gaussian_vals[i] * \\\n",
    "             std_of_neuron_variances[j] * (normalized_mean[j]**2 - 1) * standard_gaussian_vals[j]\n",
    "        output_cov_ij = t1 + t2 + t3 + t4\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(float(output_cov_ij.detach().cpu().numpy()))\n",
    "    n_features = input_covariance.shape[0]\n",
    "    output_covariance_sparse = csc_matrix((data, (rows, cols)), shape=(n_features, n_features))\n",
    "    return output_covariance_sparse\n",
    "\n",
    "def propagate_gaussian_analytically(input_mean, input_covariance_sparse, layer, mask=None):\n",
    "    # Add support for masks!\n",
    "    def softrelu(x):\n",
    "        # softrelu ≈ 0, softrelu ≈ μ/σ, or softrelu ≈ \"partial activation\"\n",
    "        return standard_gaussian(x) + x*gaussian_cdf(x)\n",
    "    \n",
    "    rows, cols, data = [], [], []\n",
    "    for i, j in mask:\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        data.append(input_covariance_sparse[i, j])\n",
    "    masked_covariance_sparse = csc_matrix((data, (rows, cols)), shape=input_covariance_sparse.shape)\n",
    "\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        output_mean = layer(input_mean)\n",
    "        W = layer.weight.detach().cpu().numpy()\n",
    "        W_sparse = csc_matrix(W)\n",
    "        temp = masked_covariance_sparse @ W_sparse.T \n",
    "        output_covariance_sparse = W_sparse @ temp\n",
    "        \n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        input_shape = input_mean.shape\n",
    "        # Flatten to vector and get stats\n",
    "        input_mean_flat = input_mean.reshape(-1)\n",
    "\n",
    "        # Extract diagonal variances from sparse matrix\n",
    "        diagonal_indices = masked_covariance_sparse.diagonal()\n",
    "        neuron_variances = torch.tensor(diagonal_indices, dtype=torch.float32)\n",
    "        std_of_neuron_variances = torch.sqrt(neuron_variances)\n",
    "        \n",
    "        # ReLU(x) = max(0, x) is hard so use softrelu(x) = φ(x) + x*Φ(x) to propagate mean\n",
    "        output_mean = std_of_neuron_variances * softrelu(input_mean_flat / std_of_neuron_variances)\n",
    "        output_mean = output_mean.reshape(input_shape)\n",
    "        \n",
    "        # ReLU changes not just individual variances, but also correlations between neurons\n",
    "        output_covariance_sparse = relu_covariance(input_mean_flat, masked_covariance_sparse, std_of_neuron_variances)\n",
    "    \n",
    "    return output_mean, output_covariance_sparse\n",
    "\n",
    "def compute_estimator_mse(true_output_distribution, predicted_output_distribution):\n",
    "    true_mean = torch.tensor(true_output_distribution.mean)\n",
    "    predicted_mean = torch.tensor(predicted_output_distribution.mean)\n",
    "    mse = torch.mean((true_mean - predicted_mean) ** 2)\n",
    "    return mse.item()\n",
    "\n",
    "def compute_estimator_kl_div(true_output_distribution, predicted_output_distribution):\n",
    "    mu_true = true_output_distribution.mean\n",
    "    mu_pred = predicted_output_distribution.mean\n",
    "    sigma_true = true_output_distribution.covariance_sparse.toarray()\n",
    "    sigma_pred = predicted_output_distribution.covariance_sparse.toarray()\n",
    "    # Compute mean difference\n",
    "    mu_diff = mu_pred - mu_true\n",
    "    # Compute KL divergence: KL(P||Q) = 0.5 * [tr(Σ_Q^(-1) * Σ_P) + (μ_Q - μ_P)^T * Σ_Q^(-1) * (μ_Q - μ_P) - k + ln(det(Σ_Q)/det(Σ_P))]\n",
    "    k = len(mu_true)\n",
    "    sigma_true_inv = np.linalg.inv(sigma_true)\n",
    "    trace_term = np.trace(sigma_true_inv @ sigma_pred)\n",
    "    quad_term = mu_diff.T @ sigma_true_inv @ mu_diff\n",
    "    log_det_term = np.log(np.linalg.det(sigma_true)) - np.log(np.linalg.det(sigma_pred))\n",
    "    kl_div = 0.5 * (trace_term + quad_term - k + log_det_term)\n",
    "    return kl_div\n",
    "\n",
    "def find_true_output_distribution(input_distribution, model, n_samples=100000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_samples = input_distribution.sample(torch.Size([n_samples]))\n",
    "        output_samples = model(input_samples)\n",
    "        output_samples_np = output_samples.cpu().numpy()\n",
    "        empirical_mean = np.mean(output_samples_np, axis=0)\n",
    "        empirical_cov = np.cov(output_samples_np, rowvar=False)\n",
    "        empirical_cov_sparse = csc_matrix(empirical_cov)\n",
    "        empirical_mean_tensor = torch.from_numpy(empirical_mean).float()\n",
    "    return SparseGaussian(empirical_mean_tensor, empirical_cov_sparse)\n",
    "\n",
    "def calculate_range_exceedance_probability(sparse_gaussian_dist, lower_bound=-5, upper_bound=5):\n",
    "    mean = sparse_gaussian_dist.mean\n",
    "    mu = mean[0] if len(mean.shape) > 0 else mean\n",
    "    variance = sparse_gaussian_dist.covariance_sparse[0, 0]\n",
    "    sigma = np.sqrt(max(variance, 1e-9))\n",
    "    prob_below = norm.cdf(lower_bound, loc=mu, scale=sigma)\n",
    "    prob_above = 1 - norm.cdf(upper_bound, loc=mu, scale=sigma)\n",
    "    return prob_below + prob_above\n",
    "\n",
    "def calculate_covariance_tracking_ratio(true_distribution, estimated_distribution):\n",
    "    true_covariance_sparse = true_distribution.covariance_sparse\n",
    "    estimated_covariance_sparse = estimated_distribution.covariance_sparse\n",
    "    \n",
    "    true_norm = np.sqrt((true_covariance_sparse.multiply(true_covariance_sparse)).sum())\n",
    "    estimated_norm = np.sqrt((estimated_covariance_sparse.multiply(estimated_covariance_sparse)).sum())\n",
    "    \n",
    "    if true_norm == 0:\n",
    "        return 1.0 if estimated_norm == 0 else 0.0\n",
    "    \n",
    "    return min(estimated_norm / true_norm, 1.0)\n",
    "\n",
    "def calculate_total_covariance_entries(model, input_dim):\n",
    "    total_entries = 0\n",
    "    current_dim = input_dim\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            total_entries += current_dim * current_dim\n",
    "            current_dim = layer.out_features\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "        #     # For ReLU, we only consider the diagonal entries\n",
    "            total_entries += current_dim * current_dim\n",
    "    total_entries += current_dim * current_dim\n",
    "    \n",
    "    return total_entries\n",
    "\n",
    "def calculate_initial_entries(model, input_dim):\n",
    "    initial_entries = 0\n",
    "    current_dim = input_dim\n",
    "    for layer in model.network:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            initial_entries += current_dim\n",
    "            current_dim = layer.out_features\n",
    "    initial_entries += current_dim\n",
    "    \n",
    "    return initial_entries\n",
    "\n",
    "def create_perfect_ordering(input_distribution, model, initial_masks):\n",
    "    # Get true output distribution for comparison\n",
    "    true_output_distribution = find_true_output_distribution(input_distribution, model)\n",
    "    \n",
    "    # Create working copies of masks, ensuring they are lists for append operations\n",
    "    current_masks = [list(mask) for mask in initial_masks]\n",
    "    optimal_ordering = []\n",
    "    \n",
    "    # Initial conditions\n",
    "    initial_mean = input_distribution.mean\n",
    "    initial_covariance = csc_matrix(input_distribution.covariance_matrix.numpy())\n",
    "    \n",
    "    # Pre-calculate layer dimensions to avoid recomputation\n",
    "    layer_dimensions = []\n",
    "    current_dim = input_distribution.mean.shape[0]\n",
    "    \n",
    "    for layer in model.network:\n",
    "        print(layer, current_dim)\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            layer_dimensions.append(current_dim)\n",
    "            current_dim = layer.out_features\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            layer_dimensions.append(current_dim)\n",
    "    # Loop indefinitely until no more beneficial entries can be found\n",
    "    while True:\n",
    "        best_kl_divergence = float('inf')\n",
    "        best_choice = None\n",
    "        \n",
    "        # Try adding entries to each layer\n",
    "        for layer_idx, layer in enumerate(model.network):\n",
    "           # if isinstance(layer, nn.Linear):\n",
    "                current_mask = current_masks[layer_idx]\n",
    "                dimension = layer_dimensions[layer_idx]\n",
    "                \n",
    "                # Try adding each possible new entry to this layer's mask\n",
    "                for i in range(dimension):\n",
    "                    for j in range(dimension):\n",
    "                        if (i, j) not in current_mask:\n",
    "                            # Create temporary masks with this new entry added\n",
    "                            temp_masks = [mask.copy() for mask in current_masks]\n",
    "                            temp_masks[layer_idx].append((i, j))\n",
    "                            \n",
    "                            # Simulate forward propagation with these temporary masks\n",
    "                            try:\n",
    "                                temp_mean = initial_mean\n",
    "                                temp_covariance = initial_covariance\n",
    "                                \n",
    "                                for temp_layer_idx, temp_layer in enumerate(model.network):\n",
    "                                    temp_mean, temp_covariance = propagate_gaussian_analytically(\n",
    "                                        temp_mean, temp_covariance, temp_layer, temp_masks[temp_layer_idx]\n",
    "                                    )\n",
    "                                \n",
    "                                # Evaluate this choice by computing KL divergence\n",
    "                                estimated_output_distribution = SparseGaussian(temp_mean.detach(), temp_covariance)\n",
    "                                kl_divergence = compute_estimator_kl_div(true_output_distribution, estimated_output_distribution)\n",
    "                                \n",
    "                                # If this choice is the best so far, record it\n",
    "                                if kl_divergence < best_kl_divergence:\n",
    "                                    best_kl_divergence = kl_divergence\n",
    "                                    best_choice = (layer_idx, i, j)\n",
    "                                    \n",
    "                            except Exception as e:\n",
    "                                # This choice resulted in an error, so skip it.\n",
    "                                # For debugging, you might want to uncomment the following line:\n",
    "                                print(f\"Skipping choice ({layer_idx}, {i}, {j}) due to error: {e}\")\n",
    "                                continue\n",
    "            # else:\n",
    "            #     # If the layer is not Linear, we skip it for mask additions\n",
    "            #     continue\n",
    "\n",
    "        \n",
    "        # After checking all possible additions, update the ordering with the best one found\n",
    "        if best_choice is not None:\n",
    "            layer_idx, i, j = best_choice\n",
    "            optimal_ordering.append(best_choice)\n",
    "            current_masks[layer_idx].append((i, j))\n",
    "            \n",
    "            if len(optimal_ordering) % 20 == 0:\n",
    "                print(f\"  Step {len(optimal_ordering)}: Added ({i}, {j}) to layer {layer_idx}, KL = {best_kl_divergence:.6f}\")\n",
    "        else:\n",
    "            # If best_choice is still None, no valid entry could be added.\n",
    "            # This means the process is complete.\n",
    "            print(f\"No more beneficial or valid entries found after {len(optimal_ordering)} additions. Terminating.\")\n",
    "            break # Exit the while loop\n",
    "\n",
    "    print(f\"Optimal ordering computed with {len(optimal_ordering)} entries.\")\n",
    "    return optimal_ordering\n",
    "\n",
    "def update_mask_with_ordering(masks, ordering, step):\n",
    "    if step < len(ordering):\n",
    "        layer_idx, i, j = ordering[step]\n",
    "        if (i, j) not in masks[layer_idx]:\n",
    "            masks[layer_idx].append((i, j))\n",
    "        if layer_idx != 0 and layer_idx % 2 == 0:\n",
    "            step = step + 1\n",
    "            masks[layer_idx-1] = masks[layer_idx]\n",
    "    return masks, step\n",
    "\n",
    "torch.manual_seed(42)\n",
    "input_mean = torch.zeros(3)\n",
    "full_covariance_matrix = torch.tensor([\n",
    "    [1.0, 0.7, 0.3],\n",
    "    [0.7, 1.5, 0.8],\n",
    "    [0.3, 0.8, 2.0]\n",
    "])\n",
    "input_distribution = MultivariateNormal(loc=input_mean, covariance_matrix=full_covariance_matrix)\n",
    "X = input_distribution.sample(torch.Size([1000]))\n",
    "true_weights = torch.tensor([[2.0], [3.0], [-1.0]])\n",
    "y = X @ true_weights\n",
    "\n",
    "# --- Model Training ---\n",
    "model = DeepNeuralNet(input_size=3, hidden_sizes=[4, 12, 4], output_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print final model architecture\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# --- Main Propagation Loop with Perfect Ordering ---\n",
    "input_dim = 3 \n",
    "total_entries = calculate_total_covariance_entries(model, input_dim)\n",
    "initial_entries = calculate_initial_entries(model, input_dim)\n",
    "print(f\"Total possible covariance entries across all layers: {total_entries}\")\n",
    "print(f\"Initial diagonal entries: {initial_entries}\")\n",
    "initial_mean = input_distribution.mean\n",
    "initial_covariance = csc_matrix(input_distribution.covariance_matrix.numpy())\n",
    "training_steps = 5000\n",
    "masks = []\n",
    "current_dim = input_dim\n",
    "for layer_idx, layer in enumerate(model.network):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        masks.append(create_diagonal_mask(current_dim))\n",
    "        current_dim = layer.out_features\n",
    "    elif isinstance(layer, nn.ReLU):\n",
    "        masks.append(create_diagonal_mask(current_dim))\n",
    "\n",
    "\n",
    "# PRE-COMPUTE THE OPTIMAL ORDERING\n",
    "print(\"Computing optimal ordering (this may take a while)...\")\n",
    "optimal_ordering = create_perfect_ordering(input_distribution, model, masks)\n",
    "print(f\"Optimal ordering contains {len(optimal_ordering)} entries\")\n",
    "training_steps = min(training_steps, len(optimal_ordering))\n",
    "print(f\"Running {training_steps} training steps\")\n",
    "kl_divergences = []\n",
    "estimated_exceedance_probs = []\n",
    "covariance_tracking_ratios = []\n",
    "percent_filled = [] \n",
    "updated_step = 0\n",
    "true_output_distribution = find_true_output_distribution(input_distribution, model)\n",
    "true_exceedance_prob = calculate_range_exceedance_probability(true_output_distribution)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for step in range(training_steps):\n",
    "        updated_step += 1\n",
    "        propagated_gaussians = []\n",
    "        current_mean = initial_mean\n",
    "        current_covariance = initial_covariance\n",
    "        if updated_step > 0:\n",
    "            masks, updated_step = update_mask_with_ordering(masks, optimal_ordering, step - 1)\n",
    "        \n",
    "        # Calculate current percentage filled\n",
    "        current_entries = initial_entries + updated_step\n",
    "        current_percent = (current_entries / total_entries) * 100\n",
    "        percent_filled.append(current_percent)\n",
    "        for layer_idx, layer in enumerate(model.network):\n",
    "            output_mean, output_covariance = propagate_gaussian_analytically(current_mean, current_covariance, layer, masks[layer_idx])\n",
    "            propagated_layer_gaussian = SparseGaussian(output_mean, output_covariance)\n",
    "            propagated_gaussians.append(propagated_layer_gaussian)\n",
    "            current_mean = output_mean\n",
    "            current_covariance = output_covariance\n",
    "        estimated_output_distribution = propagated_gaussians[-1]\n",
    "\n",
    "        # Calculate metrics for this step\n",
    "        estimator_mse = compute_estimator_mse(true_output_distribution, estimated_output_distribution)\n",
    "        kl_divergence = compute_estimator_kl_div(true_output_distribution, estimated_output_distribution)\n",
    "        estimated_exceedance_prob = calculate_range_exceedance_probability(estimated_output_distribution)\n",
    "        covariance_tracking_ratio = calculate_covariance_tracking_ratio(true_output_distribution, estimated_output_distribution)\n",
    "        kl_divergences.append(kl_divergence)\n",
    "        estimated_exceedance_probs.append(estimated_exceedance_prob)\n",
    "        covariance_tracking_ratios.append(covariance_tracking_ratio)\n",
    "        if updated_step % 100 == 0:\n",
    "            next_addition = optimal_ordering[step] if step < len(optimal_ordering) else \"None\"\n",
    "            print(f\"Step {step}: {current_percent:.2f}% filled, KL Div = {kl_divergence:.6f}, Est. Exceedance = {estimated_exceedance_prob:.6f}/{true_exceedance_prob:.6f}, Cov. Ratio = {covariance_tracking_ratio:.6f}, Next: {next_addition}\")\n",
    "\n",
    "# --- Create Visualization ---\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "ax1.plot(percent_filled, kl_divergences, 'b-', linewidth=2, label='KL Divergence')\n",
    "ax1.set_xlabel('Covariance Matrix Filled (%)')\n",
    "ax1.set_ylabel('KL Divergence')\n",
    "ax1.set_title('KL Divergence vs Covariance Matrix Fill Percentage')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "ax2.plot(percent_filled, estimated_exceedance_probs, 'r-', linewidth=2, label='Estimated Exceedance Probability')\n",
    "ax2.axhline(y=true_exceedance_prob, color='g', linestyle='--', linewidth=2, label='True Exceedance Probability')\n",
    "ax2.set_xlabel('Covariance Matrix Filled (%)')\n",
    "ax2.set_ylabel('Exceedance Probability')\n",
    "ax2.set_title('Exceedance Probability (Outside [-5, 5]) vs Covariance Matrix Fill Percentage')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "ax3.plot(percent_filled, covariance_tracking_ratios, 'm-', linewidth=2, label='Covariance Tracking Ratio')\n",
    "ax3.axhline(y=1.0, color='k', linestyle='--', linewidth=1, alpha=0.7, label='Perfect Tracking (1.0)')\n",
    "ax3.set_xlabel('Covariance Matrix Filled (%)')\n",
    "ax3.set_ylabel('Covariance Tracking Ratio')\n",
    "ax3.set_title('Covariance Tracking Ratio vs Covariance Matrix Fill Percentage')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_visualizations/gaussian_propagation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# --- Output ---\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Final Percentage Filled: {percent_filled[-1]:.2f}%\")\n",
    "print(f\"Final KL Divergence: {kl_divergences[-1]:.6f}\")\n",
    "print(f\"Final Estimated Exceedance Probability: {estimated_exceedance_probs[-1]:.6f}\")\n",
    "print(f\"True Exceedance Probability: {true_exceedance_prob:.6f}\")\n",
    "print(f\"Difference in Exceedance Probabilities: {abs(estimated_exceedance_probs[-1] - true_exceedance_prob):.6f}\")\n",
    "print(f\"Final Covariance Tracking Ratio: {covariance_tracking_ratios[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
